---
title: "Rapid literature searching"
description: |
  Using R to rapidly search the literature 
author:
  - name: Julian Flowers 
    affiliation: Public Health England
    affiliation_url: https://www.gov.uk/phe
date: "2018-10-07"
output: 
   radix::radix_article:
     keep_md: true
---


 
Searching and critical appraising the literature is an essential skill for public health practitioners. Traditionally, this involves manual searching medical or bibliographic databases or sources of grey literature, identifying relevant abstracts or full text articles, reading and appraising retrieved papers and iterating through reference lists. If you are fortunate you might have access to a librarian or information specialist who can help with searching and retrieving publications. It can be a protracted manual process, and take many months.

With the development of APIs for bibliographic databases and R packages to read them it has become possible to automate searching and literature extraction, and with mainstreamed artificial intelligence tools for natural langauge processing (NLP) it has become possible to rapidly assess large numbers of papers.

## Example

For this exercise we'll need to download the following packages

* `tidyverse`
* `tidytext`
* `myScrapers`
* `quanteda`
* `Rcrawler`
* `rvest`


<div class="layout-chunk" data-layout="l-body">

```r
library(myScrapers)

search <- "public health[MeSH] deep learning[tw]"
start <- 2000
end <- 2018
n <- 336

r_ph <- pubmedAbstractR(search = search, start = start, end = end, n = n)
```

```
Please wait...Your query is "public health"[MeSH Terms] AND deep learning[tw] AND 2000[PDAT] : 2018[PDAT]. This returns 336 abstracts. By default 1000 abstracts are downloaded. You downloaded 336 abstracts. To retrieve more set 'n =' argument to the desired value
```

```r
r_ph
```

```
# A tibble: 336 x 5
   title            abstract            journal           DOI   year 
   <chr>            <chr>               <chr>             <chr> <chr>
 1 Deep learning a… Synovial sarcoma i… Tumour biology :… 3026… 2018 
 2 Automated chest… OBJECTIVE: In this… Biomedical engin… 2979… 2018 
 3 Deep Learning t… Early detection of… Sensors (Basel, … 2978… 2018 
 4 A survey of stu… Retrieval practice… Advances in phys… 2976… 2018 
 5 Convolutional n… With the introduct… PloS one          2973… 2018 
 6 A deep belief n… Nonlinear system m… Neural networks … 2972… 2018 
 7 The Use of Smar… The use of smartph… Journal of medic… 2970… 2018 
 8 RIDDLE: Race an… Anonymized electro… PLoS computation… 2969… 2018 
 9 Machine Learnin… The ultrasound ima… BioMed research … 2968… 2018 
10 Human Activity … In recent years, h… Journal of medic… 2966… 2018 
# ... with 326 more rows
```

```r
street_view_article <- readtext::readtext("https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5740675/pdf/pnas.201700035.pdf")

street_view_article$text
```

```
[1] "Using deep learning and Google Street View to\nestimate the demographic makeup of neighborhoods\nacross the United States\nTimnit Gebrua,1 , Jonathan Krausea , Yilun Wanga , Duyun Chena , Jia Dengb , Erez Lieberman Aidenc,d,e , and Li Fei-Feia\na\n  Artificial Intelligence Laboratory, Computer Science Department, Stanford University, Stanford, CA 94305; b Vision and Learning Laboratory, Computer\nScience and Engineering Department, University of Michigan, Ann Arbor, MI 48109; c The Center for Genome Architecture, Department of Genetics, Baylor\nCollege of Medicine, Houston, TX 77030; d Department of Computer Science, Rice University, Houston, TX 77005; and e The Center for Genome Architecture,\nDepartment of Computational and Applied Mathematics, Rice University, Houston, TX 77005\nEdited by Kenneth W. Wachter, University of California, Berkeley, CA, and approved October 16, 2017 (received for review January 4, 2017)\nThe United States spends more than $250 million each year on                    of analyzing demographic trends in great detail, in real time, and\nthe American Community Survey (ACS), a labor-intensive door-to-                 at a fraction of the cost.\ndoor study that measures statistics relating to race, gender, edu-                  Recently, Naik et al. (7) used publicly available imagery to\ncation, occupation, unemployment, and other demographic fac-                    quantify people’s subjective perceptions of a neighborhood’s\ntors. Although a comprehensive source of data, the lag between                  physical appearance. They then showed that changes in these\ndemographic changes and their appearance in the ACS can                         perceptions correlate with changes in socioeconomic variables\nexceed several years. As digital imagery becomes ubiquitous and                 (8). Our work explores a related theme: whether socioeconomic\nmachine vision techniques improve, automated data analysis may                  statistics can be inferred from objective characteristics of images\nbecome an increasingly practical supplement to the ACS. Here,                   from a neighborhood.\nwe present a method that estimates socioeconomic characteris-                       Here, we show that it is possible to determine socioeconomic\ntics of regions spanning 200 US cities by using 50 million images               statistics and political preferences in the US population by com-\nof street scenes gathered with Google Street View cars. Using                   bining publicly available data with machine-learning methods.\ndeep learning-based computer vision techniques, we determined                   Our procedure, designed to build upon and complement the\nthe make, model, and year of all motor vehicles encountered in                  ACS, uses labor-intensive survey data for a handful of cities to\nparticular neighborhoods. Data from this census of motor vehi-                  train a model that can create nationwide demographic estimates.\ncles, which enumerated 22 million automobiles in total (8% of                   This approach allows for estimation of demographic variables\nall automobiles in the United States), were used to accurately                  with high spatial resolution and reduced lag time.\nestimate income, race, education, and voting patterns at the                        Specifically, we analyze 50 million images taken by Google\nzip code and precinct level. (The average US precinct contains                  Street View cars as they drove through 200 cities, neighborhood-\n∼1,000 people.) The resulting associations are surprisingly sim-                by-neighborhood and street-by-street. In Google Street View\nple and powerful. For instance, if the number of sedans encoun-                 images, only the exteriors of houses, landscaping, and vehicles on\ntered during a drive through a city is higher than the num-                     the street can be observed. Of these objects, vehicles are among\nber of pickup trucks, the city is likely to vote for a Democrat                 the most personalized expressions of American culture: Over\nduring the next presidential election (88% chance); otherwise,                  90% of American households own a motor vehicle (9), and their\nit is likely to vote Republican (82%). Our results suggest that                 choice of automobile is influenced by disparate demographic fac-\nautomated systems for monitoring demographics may effectively                   tors including household needs, personal preferences, and eco-\ncomplement labor-intensive approaches, with the potential to                    nomic wherewithal (10). (Note that, in principle, other factors\nmeasure demographics with fine spatial resolution, in close to                  such as spacing between houses, number of stories, and extent of\nreal time.                                                                      shrubbery could also be integrated into such models.) Such street\n                                                                                scenes are a natural data type to explore: They already cover\ncomputer vision | deep learning | social analysis | demography\n                                                                                   Significance\nF    or thousands of years, rulers and policymakers have sur-\n     veyed national populations to collect demographic statistics.\nIn the United States, the most detailed such study is the Amer-\n                                                                                   We show that socioeconomic attributes such as income, race,\n                                                                                   education, and voting patterns can be inferred from cars\nican Community Survey (ACS), which is performed by the US                          detected in Google Street View images using deep learning.\nCensus Bureau at a cost of $250 million per year (1). Each                         Our model works by discovering associations between cars\nyear, ACS reports demographic results for all cities and coun-                     and people. For example, if the number of sedans in a city\nties with a population of 65,000 or more (2). However, due to                      is higher than the number of pickup trucks, that city is likely\nthe labor-intensive data-gathering process, smaller regions are                    to vote for a Democrat in the next presidential election (88%\ninterrogated less frequently, and data for geographical areas with                 chance); if not, then the city is likely to vote for a Republican\nless than 65,000 inhabitants are typically presented with a lag of                 (82% chance).\n∼ 2.5 y. Although the ACS represents a vast improvement over\nthe earlier, decennial census (3), this lag can nonetheless impede              Author contributions: T.G., J.K., J.D., E.L.A., and L.F.-F. designed research; T.G., J.K.,\neffective policymaking. Thus, the development of complemen-                     Y.W., D.C., J.D., E.L.A., and L.F.-F. performed research; T.G. and J.K. contributed new\ntary approaches would be desirable.                                             reagents/analytic tools; T.G., J.K., Y.W., D.C., J.D., E.L.A., and L.F.-F. analyzed data; and\n                                                                                T.G., J.K., E.L.A., and L.F.-F. wrote the paper.\n   In recent years, computational methods have emerged as a\npromising tool for tackling difficult problems in social science.               The authors declare no conflict of interest.\nFor instance, Antenucci et al. (4) have predicted unemployment                  This article is a PNAS Direct Submission.\nrates from Twitter; Michel et al. (5) have analyzed culture using               This open access article is distributed under Creative Commons Attribution-\nlarge quantities of text from books; and Blumenstock et al. (6)                 NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).\nused mobile phone metadata to predict poverty rates in Rwanda.                  1\n                                                                                  To whom correspondence should be addressed. Email: tgebru@stanford.edu.\nThese results suggest that socioeconomic studies, too, might be                 This article contains supporting information online at www.pnas.org/lookup/suppl/doi:10.\nfacilitated by computational methods, with the ultimate potential               1073/pnas.1700035114/-/DCSupplemental.\n13108–13113 | PNAS | December 12, 2017 | vol. 114 | no. 50                                                               www.pnas.org/cgi/doi/10.1073/pnas.1700035114\n\nmuch of the United States, and the emergence of self-driving                       cle, we deployed CNN (13, 14), the most successful deep learning\ncars will bring about a large increase in the frequency with which                 algorithm to date for object classification, to determine the make,\ndifferent locations are sampled.                                                   model, body type, and year of each vehicle (Fig. 1). Using our\n   We demonstrate that, by deploying a machine vision frame-                       human-annotated gold standard images, we trained the CNN to\nwork based on deep learning—specifically, Convolutional Neu-                       distinguish between different types of cars. Specifically, we were\nral Networks (CNN)—it is possible to not only recognize vehi-                      able to classify each vehicle into one of 2,657 fine-grained cate-\ncles in a complex street scene but also to reliably determine a                    gories, which form a nearly exhaustive list of all visually distinct\nwide range of vehicle characteristics, including make, model, and                  automobiles sold in the United States since 1990 (Fig. 1). For\nyear. Whereas many challenging tasks in machine vision (such as                    instance, our models accurately identified cars (identifying 95%\nphoto tagging) are easy for humans, the fine-grained object recog-                 of such vehicles in the test data), vans (83%), minivans (91%),\nnition task we perform here is one that few people could accom-                    SUVs (86%), and pickup trucks (82%). See SI Appendix, Fig. S1.\nplish for even a handful of images. Differences between cars can                      Using the resulting motor vehicle data, we estimate demo-\nbe imperceptible to an untrained person; for instance, some car                    graphic statistics and voter preferences as follows. For each geo-\nmodels can have subtle changes in tail lights (e.g., 2007 Honda                    graphical region we examined (city, zip code, or precinct), we\nAccord vs. 2008 Honda Accord) or grilles (e.g., 2001 Ford F-150                    count the number of vehicles of each make and model that were\nSupercrew LL vs. 2011 Ford F-150 Supercrew SVT). Neverthe-                         identified in images from that region. We also include addi-\nless, our system is able to classify automobiles into one of 2,657                 tional features such as aggregate counts for various vehicle types\ncategories, taking 0.2 s per vehicle image to do so. While it classi-              (trucks, vans, SUVs, etc.), the average price and fuel efficiency,\nfied the automobiles in 50 million images in 2 wk, a human expert,                 and the overall density of vehicles in the region (see Materials\nassuming 10 s per image, would take more than 15 y to perform                      and Methods).\nthe same task. Using the classified motor vehicles in each neigh-                     We then partitioned our dataset, by county, into two subsets\nborhood, we infer a wide range of demographic statistics, socio-                   (Fig. 2). The first is a “training set,” comprising all regions that\neconomic attributes, and political preferences of its residents.                   lie mostly in a county whose name starts with “A,” “B,” or “C”\n   In the first step of our analysis, we collected 50 million Google               (such as Ada County, Baldwin County, Cabarrus County, etc.).\nStreet View images from 3,068 zip codes and 39,286 voting                          This training set encompasses 35 of the 200 cities, ∼ 15% of the\nprecincts spanning 200 US cities (Fig. 1). Using these images                      zip codes, and ∼ 12% of the precincts in our data. The second is a\nand annotated photos of cars, our object recognition algorithm                     “test set,” comprising all regions in counties starting with the let-\n[a “Deformable Part Model” (DPM) (11)] learned to automati-                        ters “D” through “Z” (such as Dakota County, Maricopa County,\ncally localize motor vehicles on the street (12) (see Materials and                Yolo County). We used the test set to evaluate the model that\nMethods). This model took advantage of a gold-standard dataset                     resulted from the training process.\nwe generated by asking humans (both laypeople, recruited using                        Using ACS and presidential election voting data for regions\nAmazon Mechanical Turk, and car experts recruited through                          in our training set, we train a logistic regression model to esti-\nCraigslist) to identify cars in Google Street View scenes.                         mate race and education levels and a ridge regression model to\n   We successfully detected 22 million distinct vehicles, compris-                 estimate income and voter preferences on the basis of the col-\ning 32% of all of the vehicles in the 200 cities we studied and 8%                 lection of vehicles seen in a region. This simple linear model is\nof all vehicles in the United States. After localizing each vehi-                  sufficient to identify positive and negative associations between\n                                                                                                                                                                             COMPUTER SCIENCES\n                                                                                         200 Cities\n                                                                                         50,0000,000 Images\n                                                                                         22,000,000 Cars Analyzed\n                                                                                                                    Fig. 1. We perform a vehicular census of 200\n                                                                                                                    cities in the United States using 50 million Google\n                                                                                                                    Street View images. In each image, we detect cars\n                                                                                                                    with computer vision algorithms based on DPM\n                                                                                         2657 Car Categories\n                                                                                                                    and count an estimated 22 million cars. We then\n               Make: Nissan       Make: Ford               Make: Honda        Make: Honda                           use CNN to categorize the detected vehicles into\n               Model: Sentra      Model: Econoline-Cargo   Model: Accord      Model: Civic                          one of 2,657 classes of cars. For each type of car, we\n               Year: 2006         Year: 2003               Year: 1994         Year: 2004\n               Body Type: sedan   Body Type: van           Body Type: sedan   Body Type: sedan\n                                                                                                                    have metadata such as the make, model, year, body\n               Trim: 1.8 s        Trim: e-150              Trim: lx           Trim: ex                              type, and price of the car in 2012. Images courtesy\n               Price: $5,417      Price: $3,778            Price:$3,591       Price:$8,773                          of Google Maps/Google Earth.\nGebru et al.                                                                                               PNAS | December 12, 2017 | vol. 114 | no. 50 | 13109\n\n                    i. White (Seattle, Washington)            ii.Black (Seattle, Washington)         iii.. Asian (Seattle, Washington)\n              100%\n              0%\n                          actual              predicted           actual            predicted               actual              predicted\n                                                                                                                                                    Fig. 2. We use all of the cities in counties start-\n                                                                                                                                                    ing with A, B, and C (shown in purple on the\n                                                                                                                                                    map) to train a model estimating socioeconomic\n                                                                                                                                                    data from car attributes. Using this model, we\n                                                                                                                                                    estimate demographic variables at the zip code\n                                                                                                                                           Train\n                                                                                                                                                    level for all of the cities shown in green. We\n                                                                                                                                           Test\n                                                                                                                                                    show actual vs. predicted maps for the percent-\n                                                                                                                                                    age of Black, Asian, and White people in Seat-\n                                                                                                                                                    tle, WA (i–iii); the percentage of people with\n                                                                                                                                                    less than a high school degree in Milwaukee, WI\n                                                                                                                                                    (iv); and the percentage of people with grad-\n                                                                                                                                                    uate degrees in Milwaukee, WI (v). (vi) Maps\n                                                                                                                                                    the median household income in Tampa, FL.\n                                                                                                                                                    The ground truth values are mapped on Left,\n    iv. Less than High school (Milwaukee, Wisconsin)       v. Graduate school (Milwaukee, Wisconsin)                    vi. Income (Tampa, Florida) and our estimated results are on Right. We\n      34%                                                                                                     $111K\n                                                                                                                                                    accurately localize zip codes with the highest\n                                                        48%\n                                                                                                                                                    and lowest concentrations of each demographic\n                                                                                                                                                    variable such as the three zip codes in Eastern\n                                                                                                                                                    Seattle with high concentrations of Caucasians,\n                                                                                                                                                    one Northern zip code in Milwaukee with highly\n      0%                                                0%                                                    $7K                                   educated inhabitants, and the least wealthy zip\n                actual                 predicted                   actual               predicted                       actual            predicted\n                                                                                                                                                    code in Southern Tampa.\nthe presence of specific vehicles (such as Hondas) and particu-                                                       tic we examined. (The r values for the correlations were as fol-\nlar demographics (i.e., the percentage of Asians) or voter prefer-                                                    lows: median household income, r = 0.82; percentage of Asians,\nences (i.e., Democrat).                                                                                               r = 0.87; percentage of Blacks, r = 0.81; percentage of Whites,\n   Our model detects strong associations between vehicle distri-                                                      r = 0.77; percentage of people with a graduate degree, r = 0.70;\nbution and disparate socioeconomic factors. For instance, sev-                                                        percentage of people with a bachelor’s degree, r = 0.58; percent-\neral studies have shown that people of Asian descent are more                                                         age of people with some college degree, r = 0.62; percentage of\nlikely to drive Asian cars (15), a result we observe here as well:                                                    people with a high school degree, r = 0.65; percentage of people\nThe two brands that most strongly indicate an Asian neighbor-                                                         with less than a high school degree, r = 0.54). See SI Appendix,\nhood are Hondas and Toyotas. Cars manufactured by Chrysler,                                                           Figs. S3–S5. Taken together, these results show our ability to esti-\nBuick, and Oldsmobile are positively associated with African                                                          mate demographic parameters, as assessed by the ACS, using the\nAmerican neighborhoods, which is again consistent with exist-                                                         automated identification of vehicles in Google Street View data.\ning research (16). And vehicles like pickup trucks, Volkswagens,                                                             Although our city-level estimates serve as a proof-of-principle,\nand Aston Martins are indicative of mostly Caucasian neighbor-                                                        zip code-level ACS data provide a much more fine-grained por-\nhoods. See SI Appendix, Fig. S2.                                                                                      trait of constituencies. To investigate the accuracy of our meth-\n   In some cases, the resulting associations can be easily applied                                                    ods at zip code resolution, we compared our zip code-by-zip code\nin practice. For example, the vehicular feature that was most                                                         estimates to those generated by the ACS, confirming a close cor-\nstrongly associated with Democratic precincts was sedans,                                                             respondence between our findings and ACS values. For instance,\nwhereas Republican precincts were most strongly associated with                                                       when we looked closely at the data for Seattle, we found that\nextended-cab pickup trucks (a truck with rear-seat access). We                                                        our estimates of the percentage of people in each zip code who\nfound that by driving through a city while counting sedans and                                                        were Caucasian closely matched the values obtained by the ACS\npickup trucks, it is possible to reliably determine whether the                                                       (r = 0.84, p < 2e − 7). The results for Asians (r = 0.77, p = 1e −\ncity voted Democratic or Republican: If there are more sedans,                                                        6) and African Americans (r = 0.58, p = 7e − 4) were similar.\nit probably voted Democrat (88% chance), and if there are more                                                        Overall, our estimates accurately determined that Seattle, Wash-\npickup trucks, it probably voted Republican (82% chance). See                                                         ington is 69% Caucasian, with African Americans mostly residing\nFig. 3 A, iii.                                                                                                        in a few Southern zip codes (Fig. 2 i and ii). As another exam-\n   As a result, it is possible to apply the associations extracted                                                    ple, we estimated educational background in Milwaukee, Wis-\nfrom our training set to vehicle data from our test set regions                                                       consin zip codes, accurately determining the fraction of the pop-\nto generate estimates of demographic statistics and voter prefer-                                                     ulation with less than a high school degree (r = 0.70, p = 8e − 5),\nences, achieving high spatial resolution in over 160 cities. To be                                                    with a bachelor’s degree (r = 0.83, p < 1e − 7), and with post-\nclear, no ACS or voting data for any region in the test set were                                                      graduate education (r = 0.82, p < 1e − 7). We also accurately\nused to create the estimates for the test set.                                                                        determined the overall concentration of highly educated inhabi-\n   To confirm the accuracy of our demographic estimates, we                                                           tants near the city’s northeast border (Fig. 2 iv and v). Similarly,\nbegan by comparing them with actual ACS data, city-by-city,                                                           our income estimates closely match those of the ACS in Tampa,\nacross all 165 test set cities. We found a strong correlation                                                         Florida (r = 0.87, p < 1e −7). The lowest income zip code, at the\nbetween our results and ACS values for every demographic statis-                                                      southern tip, is readily apparent.\n13110 | www.pnas.org/cgi/doi/10.1073/pnas.1700035114                                                                                                                                       Gebru et al.\n\n   A                    i. Actual Percent of Voters for Obama in 2008\n                                                                           B                           Republican\n                                                                                                       Democrat\n                                                                                Los Angeles, California\n                                                                                   Casper, Wyoming\n    0%          100%\n                                                                                Milwaukee, Wisconsin\n                       ii. Predicted Percent of Voters for Obama in 2008\n                                                                                 Lexington, Kentucky\n                                                                                                                    Fig. 3. Actual and inferred voting patterns. A,\n                                                                                                                    i and ii map the actual and predicted percent-\n                                                                                Birmingham, Alabama                 age of people who voted for Barack Obama\n                                                                                                                    in the 2008 presidential election (r = 0.74).\n    0%          100%\n                                                                                                                    iii maps the ratio of detected pickup trucks\n                                                                                                                    to sedans in the 165 cities in our test set. As\n                       iii. Ratio of Sedans to Extended-cab Trucks\n                                                                                                                    can be seen from the map, the ratio is very\n                                                                                                                    low in Democratic cities such as those in the\n                                                                                   Garland, Texas                   East Coast and high in Republican cities such as\n                                                                                                                    those in Texas and Wyoming. (B) Shows actual\n                                                                                                                    vs. predicted voter affiliations for various cities\n                                                                                                                    in our test set at the precinct level using our\n                                                                                                                    full model. Democratic precincts are shown in\n                                                                                                                    blue, and Republican precincts are shown in\n                                                                                    Gilbert, Arizona\n                                                                                                                    red. Our model correctly classifies Casper, WY\n                                                                                                                    as a Republican city and Los Angeles, CA as a\n                                                                                                                    Democratic city. We accurately predict that Mil-\n                                                                                                                    waukee, WI is a Democratic city except for a\n                                                                                                                                                                          COMPUTER SCIENCES\n    0.7          0.4\n                                                                                                                    few Republican precincts in the southern, west-\n                                                                                                                    ern, and northeastern borders of the city.\n   While the ACS does not collect voter preference data, our                   mates and actual electoral outcomes at the single-precinct level\nautomated machine-learning procedure can infer such prefer-                    (r = 0.57, p < 1e − 7).\nences using associations between vehicles and the voters that                     These results illustrate the ability of our machine-learning algo-\nsurround them. To confirm the accuracy of our voter pref-                      rithm to accurately estimate both demographic statistics and voter\nerence estimates, we began by comparing them with the vot-                     preferences using a large database of Google Street View images.\ning results of the 2008 presidential election, city-by-city, across            They also suggest that our demographic estimates are accurate\nall 165 test set cities. We found a very strong correlation                    at higher spatial resolutions than those available for yearly ACS\nbetween our estimates and actual voter preferences (r = 0.73,                  data. Using our approach, zip code- or precinct-level survey data\np << 1e − 7). See SI Appendix, Fig. S5. These results con-                     collected for a few cities can be used to automatically provide\nfirm the ability of our approach to accurately estimate voter                  up-to-date demographic information for many American cities.\nbehavior.                                                                         Thus, we find that the application of fully automated com-\n   While city-level data provide a general picture, precinct-level             puter vision methods to publicly available street scenes can inex-\nvoter preferences identify patterns within a particular city. By               pensively determine social, economic, and political patterns in\ncomparing our precinct-by-precinct estimates to the 2008 presi-                neighborhoods across America. By collecting surveys for a few\ndential election results, we found that our estimates continued to             cities—the type of data routinely obtained via ACS—and infer-\nclosely match the ground truth data. For instance, in Milwaukee,               ring data for other regions using our model, we can quickly deter-\nWisconsin, a very Democratic city with 311 precincts, we cor-                  mine demographic patterns.\nrectly classify 264 precincts [85% accuracy (Fig. 3B)]. Most                      As self-driving cars with onboard cameras become increasingly\nnotably, we accurately determine that there are a few Republi-                 widespread, the type of data we use—footage of neighborhoods\ncan precincts in the South, West, and Northeastern borders of                  from vehicle-mounted cameras—are likely to become increas-\nthe city. Similarly, in Gilbert, Arizona, a Republican city, we cor-           ingly ubiquitous. For instance, Tesla vehicles currently take as\nrectly classify 58 out of 60 precincts (97% accuracy), identifying             many images as were studied here every single day. It is also\none out of the two small Democratic precincts in the city (Fig.                important to note that similar data can be obtained, albeit at\n3B). And in Birmingham, Alabama, a city that is 23% Republi-                   a slower pace, using low-tech methods: for instance, by walk-\ncan, we correctly classify 87 out of the 105 precincts (83% accu-              ing around a target neighborhood with a camera and a notepad.\nracy). Overall, there was a strong correlation between our esti-               Thus, street scenes stand in contrast to the massive textual\nGebru et al.                                                                                              PNAS | December 12, 2017 | vol. 114 | no. 50 | 13111\n\ncorpora presently used in many computational social science                    state-of-the-art in object detection, DPMs (11), instead of recent algorithms\nstudies, which can be constrained by privacy and copyright con-                such as ref. 23.\ncerns that prevent individual researchers from obtaining the raw                  For DPMs, there are two main parameters that influence the running time\ndata underlying published analyses.                                            and performance, which are the number of components and the number of\n   The automated method we present could be substantially                      parts in the model. SI Appendix, Table S3 provides an analysis of the perfor-\n                                                                               mance/time tradeoff on our data, measured on the validation set. Based on\nimproved by expanding our object recognition beyond vehicles\n                                                                               this analysis, using a DPM with a single component and eight parts strikes\n(17, 18) and incorporating global image features (7, 19–21).                   the right balance between performance and efficiency, allowing us to detect\nFor instance, our experiments show that global image features                  cars on all 50 million images in 2 wk. In contrast, the best performing param-\nextracted from CNN can also be used to infer demographics. But                 eters would have taken 2 months to run and only increased average preci-\nthis approach requires more data—at least 50% of our dataset—                  sion (AP) by 4.5.\nrather than the 12% to 15% we use using our current method                        As discussed in ref. 12, we also introduce a prior on the location and size\n(see SI Appendix). The model could also be improved by inte-                   of predicted bounding boxes and use it to improve detection accuracy. Incor-\ngrating other types of data, such as satellite images (22), social             porating this prior into our detection pipeline improves AP on the validation\nnetworks (4), and economic data pertaining to local consumer                   set by 1.92 at a negligible cost. SI Appendix, Fig. S6B visualizes this prior. The\nbehavior in particular geographic regions. Nevertheless, there                 output of our detection system is a set of bounding boxes and scores where\nare many characteristics that our methodology—which relies                     each score indicates the likelihood of its associated box containing a car.\n                                                                                  We converted these scores into estimated probabilities via isotonic\non publicly available data—may not be able to infer (see SI\n                                                                               regression (24) (see SI Appendix for details). We report numbers using a\nAppendix). For instance, the age of children in a neighborhood                 detection threshold of −1.5 (applied before the location prior). At test time,\ncan be estimated with moderate accuracy (r = 0.54), while the                  after applying the location prior (which lowers detection decision values),\npercentage of farmers in a neighborhood was not successfully                   we use a detection threshold of −2.3. This reduces the average number\ninferred using our method (r = 0.0).                                           of bounding boxes per image to be classified from 7.9 to 1.5 while only\n   Although automated methods could be powerful resources                      degrading AP by 0.6 (66.1 to 65.5) and decreasing the probability mass of\nfor both researchers and policymakers, their progress will raise               all detections in an image from 0.477 to 0.462 (a 3% drop). SI Appendix,\nimportant ethical concerns; it is clear that public data should not            Fig. S8 shows examples of car detections using our model. Bounding boxes\nbe used to compromise reasonable privacy expectations of indi-                 with cars have high estimated probabilities, whereas the opposite is true for\nvidual citizens, and this will be a central concern moving forward.            those containing no cars. The AP of our final model is 65.7, and its precision\nIn the future, such automated methods could lead to estimates                  recall curve is visualized in SI Appendix, Fig. S7B. We calculate chance per-\n                                                                               formance using a uniform sample of bounding boxes greater than 50 pixels\nthat are accurately updated in real time, dramatically improving\n                                                                               in width and height.\nupon the time resolution of a manual survey.\n                                                                               Car Classification. Our pipeline, described in ref. 12, classifies automobiles\nMaterials and Methods\n                                                                               into one of 2,657 visually distinct categories with an accuracy of 33.27%.\nHere, we describe our methodology for data collection, car detection, car      We use a CNN (25) following the architecture of ref. 14 to categorize cars.\nclassification, and demographic inference. Some of these methods were par-     CNNs, like other supervised machine-learning methods, perform best when\ntially developed in an earlier paper (12), which served as a proof of concept  trained on data from a similar distribution as the test data (in our case,\nfocusing on a limited set of predictions (e.g., per capita carbon emission,    Street View images). However, the cost of annotating Street View photos\nMassachusetts Department of Vehicles registration data, income segrega-        makes it infeasible to collect enough images to train our CNN only using\ntion). Our work builds on these methods to show that income, race, educa-      this source. Thus, we used a combination of Street View and the more plen-\ntion levels, and voting patterns can be predicted from cars in Google Street   tiful product shot images as training data. We modified the traditional CNN\nView images. In the sections below, we discuss our dataset and methodology     training procedure in a number of ways.\nin more detail.                                                                   First, taking inspiration from domain adaptation, we approximated the\n                                                                               WEIGHTED method of Daumé (26) by duplicating each Street View image 10\nDataset. While learning to recognize automobiles, a model needs to be          times during training. This roughly equalizes the number of training Street\ntrained with many images of vehicles annotated with category labels. To        View and product shot images, preventing the classifier from overfitting on\nthis end, we used Amazon Mechanical Turk to gather a dataset of labeled        product shot images.\ncar images obtained from edmunds.com, cars.com, and craigslist.org. Our           Product shot and Street View images differ significantly in image resolu-\ndataset consists of 2,657 visually distinct car categories, covering all com-  tion: Cars in product shot images occupy many more pixels in the image.\nmonly used automobiles in the United States produced from 1990 onward.         To compensate for this difference, we first measured the distribution of\nWe refer to these images as product shot images. We also hired experts         bounding box resolutions in Street View images used for training. Then,\nto annotate a subset of our Google Street View images. The annotations         during training, we dynamically downsized each input image according to\ninclude a bounding box around each car in the image and the type of car        this distribution before rescaling it to fit the input dimensions of the CNN.\ncontained in the box. We partition the images into training, validation,       Resolutions are parameterized by the geometric mean of the bounding box\nand test sets. In addition to our annotated images, we gathered 50 mil-        width and height, and the probability distribution is given as a histogram\nlion Google Street View images from 200 cities, sampling GPS points every      over 35 different such resolutions. The largest resolution is 256, which is the\n25 miles. We captured 6 images per GPS point, corresponding to different       input resolution of the CNN (see SI Appendix for additional details).\ncamera rotations. Each Street View image has dimensions 860 by 573 pixels         At test time, we input each detected bounding box into the CNN and\nand a horizontal field of view of ∼90◦ . Since the horizontal field of view is obtain softmax probabilities for each car category through a single for-\nlarger than the change in viewpoint between the 6 images per GPS point,        ward pass. We only keep the top 20 predictions, since storing a full 2, 657-\nthe images have some overlapping content. In total, we collected 50,881,098    dimensional floating point vector for each bounding box is prohibitively\nGoogle Street View images for our 200 cities. They were primarily acquired     expensive in terms of storage. On average, these top 20 predictions account\nbetween June and December of 2013 with a small fraction (3.1%) obtained        for 85.5% of the softmax layer activations’ probability mass. After extensive\nin November and December of 2014. See SI Appendix for more detail on the       code optimization to make this classification step as fast as possible, we\ndata collection process.                                                       are primarily limited by the time spent reading images from disk, espe-\n                                                                               cially when using multiple GPUs to perform classification. At the most fine-\nCar Detection. In computer vision, detection is the task of localizing objects grained level (2, 657 classes), we achieve a surprisingly high accuracy of\nwithin an image and is most commonly framed as predicting the x, y, width,     33.27%. We classify the car make and model with 66.38% and 51.83% accu-\nand height coordinates of an axis-aligned bounding box around an object of     racy respectively. Whether it was manufactured in or outside of the United\ninterest. The central challenge for our work is designing an object detector   States can be determined with 87.71% accuracy.\nthat is fast enough to run on 50 million images within a reasonable amount        We show confusion matrices for classifying the make, model, body type,\nof time and accurate enough to be useful for demographic inference. Our        and manufacturing country of the car (SI Appendix, Fig. S9 A–D). Body\ncomputation resources consisted of 4 T K40 graphics processing units and       type misclassifications tend to occur among similar categories. For exam-\n200 2.1 GHz central processing unit cores. As we were willing to trade a cou-  ple, the most frequent misclassification for “coupe” is “sedan,” and the\nple of percentages in accuracy for efficiency (12), we turned to the previous  most frequent misclassification for trucks with a regular cab is trucks with an\n13112 | www.pnas.org/cgi/doi/10.1073/pnas.1700035114                                                                                                 Gebru et al.\n\nextended cab. On the other hand, there are no two makes (such as Honda                           We compute the probability of voting Democrat/Republican conditioned\nand Mercedes-Benz) that are more visually similar than others. Thus, when                     on being in a city with more pickup trucks than sedans as follows. Let\na car’s make is misclassified, it is mostly to a more popular make. Similarly,                r be the ratio of pickup trucks to sedans. We would like to estimate\nmost errors at the manufacturing country level occur by misclassifying the                    P(Democrat|r > 1) and P(Republican|r < 1):\nmanufacturing country as either “Japan” or “USA,” the two most popular\ncountries. Due to the large number of classes, the only clear pattern in the                                                                    P(Democrat, r > 1)\n                                                                                                                     P(Democrat|r > 1) =                                                  [1]\nmodel-level confusion matrix is a strong diagonal, indicative of our correct                                                                        P(r > 1)\npredictions.\n                                                                                                                                                P(Republican, r < 1)\nDemographic Estimation. In all of our demographic estimations, we use the                                          P(Republican|r < 1) =                                                  [2]\n                                                                                                                                                     P(r < 1)\nfollowing set of 88 car-related attributes: the average number of detected\ncars per image; average car price; miles per gallon (city and highway); per-                     We estimate P(Democrat, r > 1), P(Republican, r < 1), P(r > 1), and\ncent of total cars that are hybrids; percent of total cars that are electric;                 P(r < 1) as follows. Let Sd = {ci } be the set of cities with more votes for\npercent of total cars that are from each of seven countries; percent of total                 Barack Obama than John McCain. Let Ss = {cj } be the set of cities with more\ncars that are foreign (not from the USA); percent of total cars from each of                  sedans than pickup trucks. Let ns be the number of elements in Ss and let\n11 body types; percent of total cars whose year (selected as the minimum                      nd s be the number of elements in Sd ∩ Ss . Similarly, let Sp be the set of cities\nof possible year values for the car) fall within each of 5 year ranges (1990–                 with more pickup trucks than sedans, Sr the set of cities with more votes\n1994, 1995–1999, 2000–2004, 2005–2009, and 2010–2014); and percent of                         for John McCain than Barack Obama, and nr p the number of elements in\ntotal cars whose make is each of 58 makes in our dataset.                                     Sr ∩ Sp . Finally, let C be the number of cities in our test set:\n    Socioeconomic data were obtained from the ACS (2) and were collected                                                                                   nd s\nbetween 2008–2012. See SI Appendix for more detail on ground truth data                                                       P(Democrat, r > 1) ≈                                        [3]\n                                                                                                                                                            C\nused in our analysis (e.g., census codes). Data for the 2008 US presidential\nelection were provided to us by the authors of ref. 27 and consist of precinct-                                                                            nr p\nlevel vote counts for Barack Obama and John McCain. We ignore votes cast                                                     P(Republican, r < 1) ≈                                       [4]\n                                                                                                                                                            C\nfor any other person; that is, the count of total votes is determined solely by\nvotes for Obama and McCain.                                                                                                                         ns\n    To perform our experiments, we partitioned the zip codes, precincts, and                                                          P(r > 1) ≈                                          [5]\n                                                                                                                                                    C\ncities in our dataset into training and test sets as discussed in the main text,\ntraining a model on the training set and predicting on the test set. We used                                                                        np\na ridge regression model for income and voter affiliation estimation. For                                                             P(r < 1) ≈                                          [6]\n                                                                                                                                                    C\nrace and education, we used logistic regression to use structure inherent in\nthe data. Specifically, for each region, summing the percentage of people                     Using these estimates, we calculate P(Democrat|r > 1) and P(Republican|r <\nwith each of the 5 possible educational backgrounds (or each race) should                     1) according to Eqs. 1 and 2.\nyield 100%. In all cases, we trained 5 models using fivefold cross-validation\nto select the regularization parameter and averaged the trained models. We                    ACKNOWLEDGMENTS. We thank Neal Jean, Stefano Ermon, and Marshall\n                                                                                              Burke for helpful suggestions and edits; everyone who worked on anno-\nnormalize the features to have zero mean and unit SD (parameters deter-\n                                                                                              tating our car dataset for their dedication; and our friends and family and\nmined on the training set). We also clip predictions to stay within the range                 the entire Stanford Vision lab, especially Brendan Marten, Serena Yeung,\nof the training data, preventing our estimates from having extreme values.                    and Selome Tewoderos for their support, input, and encouragement. This\nThe geographical regions of interest are restricted to be ones with a popu-                   research is partially supported by NSF Grant IIS-1115493, the Stanford DARE\nlation of at least 500 and at least 50 detected cars.                                         fellowship (to T.G.), and NVIDIA (through donated GPUs).\n 1. Department of Commerce, US Census Bureau (2013) US census bureau’s bud-                         able at papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-\n                                                                                                                                                                                                 COMPUTER SCIENCES\n    get estimates. Available at www.osec.doc.gov/bmi/budget/fy13cbj/Census FY2013                   neural-networks.pdf. Accessed November 9, 2017.\n    CongressionalJustification-FINAL.pdf. Accessed September 13, 2014.                        15.   Bland M (2012) Asian consumers and the automotive market. Available at app.\n 2. Department of Commerce, US Census Bureau (2012) American community survey 5                     compendium.com/uploads/user/a33eed35-8a44-4da7-84c4-16f3751fe303/9855ee60-\n    year data (2008-2012). Available at https://factfinder.census.gov/faces/tableservices/          f764-43b4-84c4-40950ff36307/File/3e1e2e5d8d20fad49eaac919e38abc8e/polk 3af 05\n    jsf/pages/productview.xhtml?src=bkmk. Accessed September 13, 2014.                              17 2012 presentation.pdf. Accessed November 6, 2017.\n 3. Department of Commerce, US Census Bureau (2010) Decennial census. Available               16.   Auto Remarketing Staff (2011) Which brands most attract African-American buyers?\n    at https://www.census.gov/data/developers/data-sets/decennial-census.html. Access-              Available at www.autoremarketing.com/content/trends/which-brands-most-attract-\n    ed September 13, 2014.                                                                          african-american-buyers. Accessed October 24, 2016.\n 4. Antenucci D, Cafarella M, Levenstein M, Ré C, Shapiro MD (2014) Using Social Media       17.   Simo-Serra E, Fidler S, Moreno-Noguer F, Urtasun R (2015) Neuroaesthetics in fashion:\n    to Measure Labor Market Flows (National Bureau of Economic Research, Cambridge,                 Modeling the perception of beauty. Proceedings of the IEEE Conference on Computer\n    MA), Technical Report 20010.                                                                    Vision and Pattern Recognition (IEEE, New York), pp 869–877.\n 5. Michel JB, et al. (2011) Quantitative analysis of culture using millions of digitized     18.   Matzen K, Bala K, Snavely N (2017) Streetstyle: Exploring world-wide clothing styles\n    books. Science 331:176–182.                                                                     from millions of photos. arXiv:1706.01869.\n 6. Blumenstock J, Cadamuro G, On R (2015) Predicting poverty and wealth from mobile          19.   Ordonez V, Berg TL (2014) Learning high-level judgments of urban percep-\n    phone metadata. Science 350:1073–1076.                                                          tion. European Conference on Computer Vision (Springer, Boston), pp 494–\n 7. Naik N, Philipoom J, Raskar R, Hidalgo C (2014) Streetscore–Predicting the perceived            510.\n    safety of one million streetscapes. 2014 IEEE Conference on Computer Vision and           20.   Khosla A, An B, Lim JJ, Torralba A (2014) Looking beyond the visible scene. 2014 IEEE\n    Pattern Recognition Workshops (IEEE, New York), pp 793–799.                                     Conference on Computer Vision and Pattern Recognition (IEEE, New York), pp 3710–\n 8. Naik N, Kominers SD, Raskar R, Glaeser EL, Hidalgo CA (2017) Computer vision                    3717.\n    uncovers predictors of physical urban change. Proc Natl Acad Sci USA 114:7571–            21.   Zhou B, Liu L, Oliva A, Torralba A (2014) Recognizing city identity via attribute analysis\n    7576.                                                                                           of geo-tagged images. European Conference on Computer Vision (Springer, Boston),\n 9. American Association of State Highway and Transportation Officials (2013) Vehicle               pp 519–534.\n    and Transit Availability. Commuting in America 2013 (American Association of State        22.   Jean N, et al. (2016) Combining satellite imagery and machine learning to predict\n    Highway and Transportation Officials, Washington, DC), Report 7.                                poverty. Science 353:790–794.\n10. Choo S, Mokhtarian PL (2004) What type of vehicle do people drive? The role of            23.   Ren S, He K, Girshick R, Sun J (2017) Faster r-CNN: Towards real-time object detec-\n    attitude and lifestyle in influencing vehicle type choice. Transport Res Pol Pract 38:          tion with region proposal networks. IEEE Trans Pattern Anal Mach Intell 39:1137–\n    201–222.                                                                                        1149.\n11. Felzenszwalb P, Girshick R, McAllester D, Ramanan D (2010) Object detection with          24.   Barlow RE, Bartholomew DJ, Bremner J, Brunk HD (1972) Statistical Inference\n    discriminatively trained part based models. IEEE Trans Pattern Anal Mach Intell 32:             under Order Restrictions: The Theory and Application of Isotonic Regression (Wiley,\n    1627–1645.                                                                                      New York).\n12. Gebru T, et al. (2017) Fine-grained car detection for visual census estimation in AAAI,   25.   LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to\n    in press.                                                                                       document recognition. Proc IEEE 86:2278–2324.\n13. LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning applied to          26.   Daumé H III (2007) Frustratingly easy domain adaptation. Conference of the Associ-\n    document recognition. Proc IEEE 86:2278–2324.                                                   ation for Computational Linguistics (ACL, Prague, Czech Republic).\n14. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convo-      27.   Ansolabehere S, Palmer M, Lee A (2014) Precinct-Level Election Data. Available at\n    lutional neural networks. Advances in Neural Information Processing Systems. Avail-             hdl.handle.net/1903.1/21919. Accessed January 13, 2015.\nGebru et al.                                                                                                         PNAS | December 12, 2017 | vol. 114 | no. 50 | 13113\n"
```

```r
supp_text <- readtext::readtext("http://www.pnas.org/content/pnas/suppl/2017/11/27/1700035114.DCSupplemental/pnas.1700035114.sapp.pdf")
supp_text$text %>% head()
```

```
[1] "Supporting Information\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\nTable of Contents\n1 Image Data                                                                                                                             2\n   Car Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      2\n   Product Shot Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       2\n   Street View Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      2\n        Selecting GPS Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       2\n        Sampling Images from Street View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           3\n        Annotations on Amazon Mechanical Turk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              3\n        Expert Class Annotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         3\n   Car Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      3\n   Dataset Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         4\n2 Demographic Data                                                                                                                       4\n   Income . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    4\n   Education . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     4\n   Race . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    4\n   Voting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    4\n3 Additional Details for Car Detection and Classification                                                                                4\n   Isotonic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     4\n   Additional Design Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .        4\n        Car Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      4\n        Car Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     4\n4 Raw Correlations Between Car Attributes and Demographics                                                                               4\n5 Cross Validated Performance with Randomly Split Training Data                                                                          5\n6 City Car Attributes                                                                                                                    5\n7 Additional ACS Variables                                                                                                               5\n8 Alternate Sources of Data                                                                                                              6\n   Department of Motor Vehicles Registration Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .            6\n   Satellite Night Lights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    7\n9 Related Work Using Google Street View                                                                                                  7\n10 Baselines                                                                                                                             7\n        Projecting Course Census Data to Fine Geographic Locations . . . . . . . . . . . . . . . . . . . . . . . . . . . .               7\n        Pretrained Convolutional Neural Network Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             8\n11 Timelapse Experiments–Inferring Demographics Across Time                                                                              8\n12 Sources of Error                                                                                                                      9\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                  1 of 58\n\n1. Image Data                                                       incentivized to list the exact type of car they are selling. While\n                                                                    these users are not necessarily car experts, they have detailed\nIn this section, we provide additional detail on the methodology\n                                                                    knowledge about their own car. In the case of cars.com, car\nused to acquire annotated image data for our study. This data\n                                                                    categories are represented in a very structured format. Thus,\nis required for two steps: to train computer vision models\n                                                                    after establishing a mapping between our categories and their\nthat detect and classify cars, and to apply these models on\n                                                                    format, we were able to simply scrape images for each category.\nStreet View images of cities of interest. This section proceeds\n                                                                    For craigslist.org, we scraped posts from the “cars+trucks”\nby detailing how we obtained a comprehensive list of car\n                                                                    listings of a variety of U.S. regions, and parsed the post titles\ncategories, collected a large number of “product shot” images\n                                                                    to determine which of our categories the posts belonged to.\nused to train our car classifier, gathered 50 million Street\n                                                                    Since these images are from websites with the purpose of\nView images used in our analysis, and annotated a subset\n                                                                    selling cars, we call them “product shot” images.\nfor training and verifying our model. We conclude with a\n                                                                        Some product shot images show the car from an extremely\ncomplete description of the acquired metadata for each car\n                                                                    close-up angle. Others only depict the interior of the car\ncategory.\n                                                                    (Fig. S12). Since our purpose is to recognize cars in Google\n                                                                    Street View images, our training set should have cars from\nCar Categories. The first step in assembling a dataset of an-\n                                                                    view points that can appear in Street View. Thus, we filtered\nnotated car images is grouping cars into sets of visually indis-    out images which do not contain one central automobile, with\ntinguishable classes. For example, while a 2003 Honda Accord        its exterior depicted in its entirety. Since this task is relatively\ncoupe ex and a 2005 Honda Accord coupe ls special edition are       simple, we crowdsourced it via AMT, using (3) for quality\nmanufactured in different years and have different trims (ex vs     control. Our interface for this task is shown in Fig. S13.\nls special edition), their exteriors look identical. Thus, these\n                                                                        In the final annotation step, we collected a bounding box (an\ntwo cars should be grouped into the same class. Ideally, the set\n                                                                    axis-aligned rectangle tightly enclosing the object of interest)\nof classes would contain every type of car in common use. (1)\n                                                                    around the car in each image. This ensures that our car\npresents a workflow to perform this grouping at minimal cost.\n                                                                    classifier is trained using visual information only from the car\n    We first retrieved an initial list of 15,213 car types from     itself and not extraneous background. Bounding boxes were\nthe car website Edmunds.com, collected in August 2012. This         collected using the labeling methodology and UI of (4), but\nforms a generally complete list of all cars commonly used           without the step for determining if there is more than one car\nin the United States that were produced from 1990 onward.           in the image. That step is not necessary because the output\nThroughout this document we use the term “car” to refer to          of the previous AMT task ensures that each image contains\nall types of automobiles with four wheels, including sedans,        exactly one prominent car.\ncoupes, trucks, vans, SUVs, etc., but not including e.g. semi-          Since some types of cars have many more images than\ntrucks or buses.                                                    others, we stopped annotating images for each category after\n    As a first step toward grouping these categories into a         collecting 200 labeled photos. Our goal is to build a model\nsmaller number of visually distinct classes, we used Amazon         that can recognize as many types of cars as possible. Given\nMechanical Turk (AMT) to determine whether certain pairs            our limited budget, it is more important to collect annotations\nof the 15k car types were distinguishable. The interface is         for categories with few labeled images than for those with\nshown in Fig. S10. Within each task we gave six pairs of            many annotated photos.\ncategories and the user was prompted to determine 1) if the             In the final step, we removed categories that do not have\ntwo classes had any visual differences, and 2) if they were         at least three disparate sources of data per class. We define\ndifferent, on which parts they differed. Within each task we        one source of data as one post on any of the websites we used.\nhad two pairs for which we already knew the correct answer (as      This process resulted in our final dataset consisting of 2,657\ndetermined by hand), and we required that each user on AMT          car categories.\nget the answer for those pairs correct in order to count their\nresponse. Photos for this task were acquired from the handful       Street View Images. This section outlines our methodology for\nof example images that Edmunds.com provides. The authors            collecting approximately 50 million Google Street View images\ncleaned up the data by hand, resulting in 3,141 categories          and annotating a subset of them to train our car detector\nof cars, with extremely subtle differences between these fine-      and classifier. The process includes selecting GPS (latitude,\ngrained categories. Fig. S11 shows two examples of classes          longitude) points of interest, collecting images for each of\nwith their constituent groups.                                      these points, enclosing cars in a subset of these images with\n                                                                    bounding boxes, and annotating the type of car contained in\nProduct Shot Images. After assembling a list of categories          each box. The final step is performed by car experts.\nconsisting of visually indistinguishable sets of cars, we collected\ntraining images for each class. These are annotated images          Selecting GPS Points. Before gathering Google Street View im-\ncontaining the car of interest. A commonly used method in the       ages, we first have to determine which geographical (latitude,\ncomputer vision community is to perform web image searches          longitude) points we want to collect photos for. We call each\nfor each category and cleanup the query images by hand to           latitude, longitude pair a GPS point. First, we select 200\nensure that they contain the category of interest (2). However,     cities for our analysis. These are the two largest cities in each\nthe large number of classes in our dataset makes it infeasible      state and the next 100 largest cities in the United States as\nto manually perform this task.                                      determined by population (see Tab. S1 for a complete list).\n    In order to collect training data in a scalable manner, we      For each city, we sample potential points of interest within\nleveraged e-commerce websites. We crawled images from               a square grid of length 20km, centered on one point known\ncars.com and craigslist.org, two sites where users are heavily      to lie within the city. There is a 25 meter spacing between\n2 of 58                  Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\npoints. We reverse geocode each of these points to determine     tators were only shown cars in bounding boxes whose height\nwhether they lie within the city of interest and how far away    exceeded 50 pixels. 32.89% of bounding boxes in our dataset\nthey are to the nearest road. We keep all points within 12.5     fulfill this criteria. The annotation task itself proceeded hierar-\nmeters of the nearest road. This process did not provide full    chically: Fig. S15 shows the user interface for the task. Given a\ncoverage for a handful of cities. Thus, we augmented these       Street View bounding box, annotators were first asked to select\npoints with GPS samples from road data provided by the U.S.      the make of the car (Fig. S15(A)). They were then presented\nCensus Bureau (5).                                               with a list of body types for the chosen make (Fig. S15(B)).\n                                                                 After selecting the right body type, experts were shown a list\nSampling Images from Street View. For each GPS point, we at-\n                                                                 of options for the car model, and finally, the trims and years\ntempt to sample 6 images from Google Street View, one for        associated with each model.\neach of 6 different camera rotations. This was done via browser      Since differences between categories can be extremely sub-\nemulation and requires only the latitude and longitude of each   tle at that final level, we also provided example images\npoint. However, we cannot immediately use photos retrieved       from each trim and year grouping for the annotator’s benefit\nwith this process as they appear warped: an equirectangular      (Fig. S15(C)). At any point in the process, the annotator could\nprojection is applied to images in a spherical panorama. We      declare that he or she did not have enough information to\napply the reverse transformation before all subsequent tasks     make a selection. Thus, each label at this finest level of detail\nusing the images (see Fig. S14 for an example).                  represents a confident selection by a car expert. We collected\nAnnotations on Amazon Mechanical Turk. While our product shot    a total of 69,562 car category annotations in this manner.\nimages can be used to train a car classifier, we cannot utilize\nthem to train a car detector: a model that learns to localize    Car Metadata. In addition to the images, category labels, and\nall the cars in an image. This is because all of our product     bounding boxes, we also have metadata pertaining to each\nshot images include only one prominently featured car in each    class, listed below.\nimage.\n                                                                   • Make: The make of the car, of 58 possible makes. The\n    Using the system of (4), we collected bounding box anno-\n                                                                       makes we consider are: Acura, AM General, Aston\ntations in a subset of our Street View images. To increase\n                                                                       Martin, Audi, Bentley, BMW, Buick, Cadillac, Chevro-\nthe efficiency of this process, we first filtered out all images\n                                                                       let, Chrysler, Daewoo, Dodge, Eagle, Ferrari, Fiat,\ncontaining either zero or more than 10 cars via AMT, using the\n                                                                       Fisker, Ford, Geo, GMC, Honda, Hummer, Hyundai,\nsame interface and pipeline described in the section pertaining\n                                                                       Infiniti, Isuzu, Jaguar, Jeep, Kia, Lamborghini, Land\nto product shot images. A randomly selected subset of 399,331\n                                                                       Rover, Lexus, Lincoln, Lotus, Maserati, Maybach, Mazda,\nStreet View images were annotated in this manner. We found\n                                                                       McLaren, Mercedes-Benz, Mercury, Mini, Mitsubishi, Nis-\nthat 26.6% of images were annotated as having no visible cars\n                                                                       san, Oldsmobile, Panoz, Plymouth, Pontiac, Porsche,\nand 12.4% had more than 10 cars. The distribution of the\n                                                                       Ram, Rolls-Royce, Saab, Saturn, Scion, Smart, Subaru,\nnumber of cars in the remaining images is shown in Fig. S6A.\n                                                                       Suzuki, Tesla, Toyota, Volkswagen, and Volvo.\n    Fig. S6B plots bounding box size versus location. Cars\nlocated closer to the bottom of the image tend to occupy           • Model: The model of the car, of 777 possible models.\nmore space than those near the top. This agrees with the\nintuition that cars lower in the image are closer to the camera    • Year: The manufacturing year of the automobile. Since\nand therefore appear larger. Similarly, Fig. S6C shows a               cars might not change appearance over a small number\nheatmap of bounding box location for cars in Street View.              of years, this is typically listed as a range of years. The\nMost automobiles are located near the horizon line because             minimum year in our dataset is 1990, and the maximum\nthat part of the image occupies more 3D space, i.e., more space        year is 2014.\nin the real world. There is a sharp dropoff in the distribution\nof cars above the horizon line.                                    • Body Type: The body type of the car. The 11 possible\n                                                                       values are: convertible, coupe, hatchback, minivan, sedan,\nExpert Class Annotations. To learn to recognize automobiles in         SUV, truck (regular-sized cab), truck (extended cab),\nStreet View images, a classifier needs to be trained with cars         truck (crew cab), wagon, and van.\nfrom these images. To this end, we labeled a subset of the\nbounding boxes from Street View images with the types of           • Country: The manufacturing country of the automobile.\ncars contained in them. This annotated data also enables us            The 7 possible countries are: England, Germany, Italy,\nto quantitatively evaluate how well our classifier works. In           Japan, South Korea, Sweden, and USA.\ncontrast to product shot images, we do not know the types\nof cars contained in Street View photos. Therefore, we hired       • Highway MPG: The typical miles per gallon of the car\nexpert car annotators to label these images. Experts were pri-         when driven on highways. If a class contains cars with\nmarily solicited via Craigslist ads. Those who were interested         multiple years, it is annotated with the highway MPG of\nin performing our task were first asked to annotate cars in            the oldest car in the group.\nStreet View images for one hour, and only those who could\n                                                                   • City MPG: The typical miles per gallon of the car when\nannotate at a speed of 1 car per minute and a precision of at\n                                                                       driven on non-highway streets.\nleast 80% were allowed to annotate further. 110 expert human\nannotators worked for a total of approximately two thousand        • Price: the price of the car in 2012.\nhours to label our images.\n    Very small images typically do not contain enough visual         This metadata was acquired via Edmunds.com in August\ninformation to discriminate fine levels of detail. Thus, anno-   2012, with some missing data (a handful of car prices) filled\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                           3 of 58\n\nin by car experts afterward. In cases where a class consists of    similarity of at least 0.5 with a ground truth car bounding box),\nmultiple visually indistinguishable types of cars, it is annotated isotonic regression solves the following optimization problem:\nwith the metadata of the oldest car in the set.\n                                                                                           Pn\nDataset Summary. Tab. S2 provides a summary of the annota-\n                                                                              minimize        i=1\n                                                                                                  kyi − pi k22\n                                                                               p1 ,...,pn                                         [1]\ntions collected for both product shot and Street View images,               subject to         pi ≤ pi+1 ,     1≤i≤n−1\nwhich we split into training (50%), validation (10%), and test\n(40%) sets for use in training our car detector and classifier.    Given a new detection score, a probability is estimated by\n                                                                   linear interpolation of the pi . We plot the learned mapping\n                                                                   from detection scores to probabilities in Fig. S7A.\n2. Demographic Data\nIncome. Data for median household income was obtained from         Additional Design Considerations.\nthe American Community Survey (ACS) (6), and was collected\nbetween 2008-2012. We used census variable B19013_001E,            Car Detection. We made a number of additional design choices\n“Median household income in the past 12 months (in 2013            while training and running this car detector in practice. First,\ninflation-adjusted dollars)”.                                      we only detected cars that are 50 pixels or greater in width\n                                                                   and height. The output of our detector is fed into the input of\nEducation. Education data was also obtained from the ACS (6).      our car classifier. Thus, detected cars need to have sufficient\nEducation levels are split into the following mutually exclusive   resolution and detail to enable the classifier to differentiate\ncategories (census codes in parentheses):                          between 2,657 categories of automobiles. Similarly, we trained\n                                                                   our detector using cars with greater than 50 pixels width and\n  • Less than high school graduate (B06009_002E)                   height. Our DPM is trained on a subset of 13,105 bounding\n                                                                   boxes, reducing training time from a week (projected) to 15\n  • High          school    graduate    (includes    equivalency)  hours. Using this subset instead of all ground truth bounding\n      (B06009_003E)                                                boxes results in negligible changes in accuracy.\n  • Some college or associate’s degree (B06009_004E)               Car Classification. One further challenge while classifying Street\n                                                                   View images is that our input consists of noisy detection\n  • Bachelor’s degree (B06009_005E)\n                                                                   bounding boxes. This stands in contrast to what would oth-\n  • Graduate or professional degree (B06009_006E)                  erwise be the default for training a classifier – ground truth\n                                                                   bounding boxes that are tight around each car. To tackle this\nRace. Racial demographic data was also obtained from the           challenge, we first measured the distribution of the intersection\nACS (6), and corresponds to census codes B02001_002E               over union (IOU) overlap between bounding boxes produced\n(“White alone”), B02001_003E (“Black or African American           by our car detector and ground truth boxes in the validation\nalone”), and B02001_005E (“Asian alone”).                          data. Then, we randomly sampled the Street View image\n                                                                   region input into the CNN according to this IOU distribution.\nVoting. Data for the 2008 U.S. presidential election was pro-      This simulates detections as inputs to the CNN and ensures\nvided to us by the authors of (7) and consists of precinct-level   that the classifier is trained with similar images to those we\nvote counts for Barack Obama and John McCain. For all of           encounter during testing.\nour analyses, we ignore votes cast for any other person, i.e. the\ncount of total votes is determined solely by votes for Obama       4. Raw Correlations Between Car Attributes and Demo-\nand McCain. We visualize this raw data in Fig. S16.                    graphics\n    Obama received greater than 50% of the votes in most of\nthe precincts in our dataset. This can partially be attributed     Fig. S2 shows the magnitude of weights learned by our model\nto the fact that he won the popular vote in the 2008 election.     for inferring various demographic attributes. That is, we sort\nPrecincts in our dataset are also located in major cities which    the coefficients of the regression model in descending order.\nfavor candidates from the Democratic party. Interestingly,         Each coefficient is uniquely associated with one of the 88\nObama received an extremely high percentage (≥ 95%) of the         car features used in our model. We then plot the top 5 and\nvotes in many precincts in our dataset. A large portion of         bottom 5 values. However, looking at the model weights may\nthese precincts have high concentrations of African Americans,     not always be informative since some car features are highly\nwho overwhelmingly voted for him during the 2008 election.         correlated (e.g. Lamborghini and car price). A linear model\n                                                                   distributes the magnitude of its coefficients among highly\n                                                                   correlated features. Thus, the weights of highly predictive\n3. Additional Details for Car Detection and Classifica-            car features might still be small. Thus, to investigate the\n    tion                                                           relationship between various demographic variables and cars,\nIsotonic Regression. Our car detection model outputs bound-        we list the raw correlations between all of our car features and\ning boxes and scores associated with each box. We use isotonic     ground truth demographic data.\nregression to convert these scores to probabilities depicting\nthe likelihood of containing a car. Isotonic regression learns a       Income. Correlations between median household income\nprobability for each detection score subject to a monotonicity     and each of our car attributes are given in Tab. S4. The\nconstraint. Concretely, after sorting n validation detection       five car attributes that correlate most positively with median\nscores s1 , . . . , sn such that si ≤ si+1 , and with yi a binary  household income are %Foreign (r=0.47), %Country: Japan\nvariable denoting whether detection i is correct (has Jaccard      (r=0.45), Price (r=0.44), %Make: Lexus (0.44), and %Country:\n4 of 58                     Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nGermany (r=0.43). The five car attributes that correlate most     percentage of Whites, r = 0.76; percentage of people with a\nnegatively with median household income are %Country: USA         graduate degree, r = 0.78; percentage of people with a bache-\n(r=-0.47), %Year: 1995-1999 (r=-0.42), %Make: Buick (r=-          lor’s degree, r = 0.76, percentage of people with some college\n0.40), %Make: Oldsmobile (r=-0.40), and %Make: Dodge              degree, r = 0.67, percentage of people with a high school\n(r=-0.38).                                                        degree, r = 0.76; percentage of people with less than a high\n                                                                  school degree, r = 0.73; percentage of people who voted for\n    Education. We show correlations between each of our           Barack Obama during the 2008 presidential election, r = 0.67.\ncar attributes and education levels in Tab. S5, Tab. S6, Tab. S7,\nTab. S8, and Tab. S9, and the five car attributes that correlate  6. City Car Attributes\nmost positively and most negatively with each race are given\n                                                                  Using our car detections, we can answer specific questions\nin Tab. S10.\n                                                                  about cars and cities. For example, we can ask what the\n                                                                  average age of a car on the road is, what the average car\n    Race. Correlations between our car attributes and the         price is (in the US as a whole and in each city), which city\npercentage of each race considered (White, Black, and Asian)      has the most expensive cars on average (New York, NY), or\nare given in Tab. S11, Tab. S12, and Tab. S13, respectively.      the highest percentage of foreign cars (San Francisco, CA -\nThe five car attributes that correlate most positively and most   60.02%), etc...We show maps comparing a subset of these at-\nnegatively with each race are given in Tab. S14.                  tributes across our 200 cities (average car price, the percentage\n                                                                  of foreign cars, BMWs, Chevrolets, Toyota Prius, and Ford\n    Voting. We show correlations between %Obama and all           F-150s) in Fig. S19.\nof our car-centric variables in Tab. S15, and plot our predic-\ntions versus actual voting percentages in Fig. S18. The five car  7. Additional ACS Variables\nattributes that correlate most positively with Obama’s per-\n                                                                  In this section, we report results for an additional 28 ACS\ncent of votes are Body Type: Sedan (r=0.48), #Cars/Image\n                                                                  variables that were inferred using our Google Street View based\n(r=0.37), MPG Highway (r=0.33), and MPG City (r=0.26).\n                                                                  methodology. While the ACS has many variables, we obtained\nThe five car attributes that correlate most negatively are Body\n                                                                  a subset of 28 attributes that are indicators of income, race,\nType: Crew Cab (r=-0.48), Body Type: Extended Cab (r=-\n                                                                  education and occupation levels as well as other characteristics\n0.43), Body Type: Regular Cab (r=-0.30), Price (r=-0.28),\n                                                                  of neighborhoods such as owner occupancy of housing units.\nand Body Type: SUV (r=-0.22).\n                                                                  Fig. S20 shows scatter plots of actual vs. predicted values\n                                                                  (cross validated performance with randomly split training\n5. Cross Validated Performance with Randomly Split                data).\n    Training Data                                                     Some variables can be inferred with high accuracy using our\nIn the main text, we chose zip codes and precincts in counties    methodology (e.g the Pearson correlation coefficient between\nstarting with “A”, “B” or “C” to train our model and evaluated    actual vs. predicted values for median household income for\nthe model on the rest of our data. This was done to show          units with a mortgage is r = 0.80). Variables such as the\nthat we could train a model that can infer demographics with      age of one’s children can be inferred with moderate accuracy\nreasonable accuracy, using very little data (approximately 10%    (r = 0.54). On the other hand the Farming variable cannot\nof our data). We also wanted to ensure that zip codes in the      be inferred from cars at all r = 0.0 (p \034 1e − 7 for all\nsame city were not used in training and testing.                  variables). This is in part due to the fact that our car features\n    Below, we present demographic inference results using a       reflect percentages and therefore are most suited to infer the\ndifferent training methodology. We randomly partitioned our       percentage of something as opposed to the actual value. E.g.,\nzip codes and precincts into five sets, iteratively training a    we have setup our methodology to infer the percentage of\nmodel on four of the parts and predicting on the held out set.    inhabitants in a neighborhood with a bachelor’s degree as\nAs before, we normalize the car features to have zero mean        opposed to the total number of citizens who have obtained a\nand unit standard deviation (parameters determined on the         bachelor’s degree. Note that we also do not include all vehicles\ntraining set of four parts). We furthermore clip predictions to   in our dataset (e.g. we omit tracktors and large trucks).\nbe within the range of the current training data, preventing      Including these might improve our accuracy in estimating\npredictions from becoming too extreme. In all experiments         farming related ACS variables. It is also important to note\nat the zip code level we restricted the zip codes used to be      that we have not refined our methodology to be able to infer\nones with a population of at least 5,000 and at least 500         these additional variables and have simply applied our current\ndetected cars, which reduces the number of zip codes under        methodology to them and reported our results. However, some\nconsideration from 3,068 to 2,430, mostly as a result of the      of these results indicate that not all ACS variables can be\nrestriction on the number of detected cars.                       inferred from analyzing cars in Google Street View images.\n                                                                  Thus, our methodology is only applicable to those variables\n    Fig. S17 and Fig. S18 show scatter plots and Pearson cor-\n                                                                  that are most strongly correlated with car preferences.\nrelation coefficients for actual vs. predicted values of income,\n                                                                      This additional data was also obtained using the ACS\neducation, race and voting patterns respectively. The scatter\n                                                                  API (6). We list the variables of interest below (census codes\nplots show results at the highest level of spatial granularity\n                                                                  in parentheses):\nour analysis is performed in (precinct level for voting patterns\nand zip code level for everything else). (The r-values for the      • Median Age by Sex-Total (B01002_001E)\ncorrelations were: median household income, r = 0.79; per-\ncentage of Asians, r = 0.79; percentage of Blacks, r = 0.81;        • Median Age by Sex-Male (B01002_002E)\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                            5 of 58\n\n  • Median Age by Sex-Female (B01002_003E)                       released by the Massachusetts DMV, the only state to release\n                                                                 extensive vehicle registration data (8). We measured the Pear-\n  • Median Household Income by Age of Householder-Total          son correlation coefficient between each detected and registered\n     (B19049_001E)                                               make’s distribution across zip codes (Fig. S21 (A)). Twenty\n                                                                 five of the top thirty makes have a Pearson’s r correlation\n  • Housing Units-Total (B25001_001E)\n                                                                 of r>0.5. Conversely, classifying according to the 2011 na-\n  • Occupancy Status-Total (B25002_001E)                         tional auto sales distribution (9) results in correlation r=0\n                                                                 with DMV data. Beyond Massachusetts, we measure the cor-\n  • Occupancy Status-Occupied (B25002_002E)                      relation between our detected car make distribution and the\n                                                                 2011 national distribution of car makes as r=0.97. Fig. S21 (B)\n  • Occupancy Status-Vacant (B25002_003E)                        plots the DMV values vs. our predicted percentages for the\n                                                                 distribution of Hondas in each zip code. Fig. S22 —Fig. S23\n  • Median Number of Rooms-Median number of rooms\n                                                                 show the latter plot for all makes instead of only Hondas.\n     (B25018_001E)\n                                                                    Vehicle data for Massachusetts is available from the Mas-\n  • TENURE BY UNITS IN STRUCTURE-Owner-occupied                  sachusetts Vehicle Census (8) and contains anonymized zip\n     housing units (B25032_002E)                                 code and model information for all vehicles registered in Mas-\n                                                                 sachusetts between 2008 and 2011. Since our comparison with\n  • TENURE BY UNITS IN STRUCTURE-Renter-occupied                 this data was done at the make level, aligning their list of\n     housing units (B25032_013E)                                 car classes and ours entailed only aligning the list of makes,\n                                                                 which was done by hand. We performed our experiments on\n  • Median household income for units with a mortgage            the intersection of detected and registered makes resulting in\n     (B25099_002E)                                               a total of 45 makes.\n  • Median household income for units without a mortgage            To calculate the distribution of car makes in each zip code,\n     (B25099_003E)                                               we compute the expected number of each of the 2,657 car\n                                                                 classes across the zip code, then simply use the make metadata\n  • Bedrooms-Total (B25041_001E)                                 associated with each car class to calculate the expected number\n                                                                 of cars for each make within the zip code. Since the expected\n  • Total Population (B01003_001E)                               number of instances of a particular car across a zip code is the\n                                                                 sum of the expected number of instances of the car across all\n  • Total Race (B02001_001E)\n                                                                 images within that zip code, the problem reduces to calculating\n  • Total Education (B06009_001E)                                this expectation for a single image.\n                                                                    With I an image and c one of the 2,657 classes, we decom-\n  • American Indian and Alaska Native alone (B02001_004E)        pose the expectation for a single image as\n                                                                                          X\n  • aggregate number of vehicles for travel (B08015_001E)               E[#class c|I] =         P (car|b, I)P (class c|car, b, I) [2]\n                                                                                         bbox b\n  • age of own children (B05009_001E)\n                                                                 where we are summing over all bounding boxes b for generic\n  • own children under 6 years (B05009_002E)\n                                                                 cars detected by our model. We model P (car|b, I) using iso-\n  • 6-17 years (B05009_020E)                                     tonic regression (described in Methods), and P (class c|car, b, I)\n                                                                 corresponds to the conditional probabilities output by the soft-\n  • management (B24021_002E)                                     max layer of our CNN classifier.\n                                                                    To obtain the percentage of each make, we aggregate these\n  • service (B24021_018E)                                        category-level expectations by car make and compute percent-\n                                                                 ages using the make-level expectations.\n  • farming (B24021_030E)\n                                                                    The Pearson coefficient for each make is calculated by taking\n                                                                 the percentage of that make in one zip code as a single data\n8. Alternate Sources of Data                                     point. We chose zip codes with greater than 5,000 inhabitants\nDepartment of Motor Vehicles Registration Data. Cars in          and 500 detected cars in the three Massachusetts cities in our\nGoogle Street View images capture the types of automobiles       dataset (Boston, Springfield, Worcester), resulting in a total of\nthat are parked, or pass through a neighborhood in a given       37 zip codes. For all experiments, we used registration records\nsnapshot of time. If it is near a freeway or a parking lot, a    that were valid during the second quarter of 2010. As outlined\nhigh density of cars will be detected. In our work, we use       by (8), registration datasets are most complete at that point.\nthis information to infer the demographic characteristics of        Fig. S24A shows the Pearson correlation coefficient between\nneighborhoods. How do our detected automobiles compare to        the distribution of registered and detected cars across zip codes\nthose from DMV data? We do not expect them to be exactly         for all makes that are in the intersection of our dataset and\nthe same because cars in Google Street View do not always        Massachusetts registration data: in contrast to Fig. S21 (A),\nbelong to inhabitants of the neighborhood they are captured      this shows all makes instead of the top 30. In addition to\nin.                                                              comparing the distribution of each registered and detected\n    We compared the distribution of cars we detect in Street     make across zip codes, we performed two additional experi-\nView images with the distribution in Boston, Worcester and       ments. First, we compared the distribution of all detected\nSpringfield, MA (the three Massachusetts cities in our dataset), and registered makes per zip code, computing the Pearson\n6 of 58                Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\ncorrelation coefficient for each zip code (Fig. S24B). All zip    is unclear. However, as discussed in the main text, our paper\ncodes have correlation greater than 0.8. In contrast, classifying presents a proof of concept that can be expanded upon. Our\naccording to the national distribution only results in correla-   future work plans to incorporate all other publicly available\ntions greater than 0.45 for all zip codes. Next, we compared      data (including night time lights, CNN features, other de-\nthe total distribution of registered and detected cars in all 37  tected objects such as trees and pedestrians) into our model\nzip codes (Fig. S25) and measured a correlation coefficient of    to improve its accuracy.\n0.94. Prediction using the national sales distribution instead\nof our approach only has correlation 0.82.                        9. Related Work Using Google Street View\n    Since we have very few zip codes with DMV data, we\ndid not train a model using DMV data to infer demographics.       A number of prior works have asked similar questions to our\nHowever, our experiments show that there is significant overlap   work about cities. Salesses et al. (18) collected a dataset of\nbetween the information we collect and DMV data. But we           approximately 1M Google Street View images labeled with\ndo not capture exactly the same information. The question         annotators’ perception of the safety, uniqueness and wealth of\nwe ask is how the look of a neighborhood, as captured by          the locations portrayed by each image. Subsequent works (19–\nthe cars that one sees in it, is related to demographics. If      21) infer these labels using global image features (20) and\nthe neighborhood is near a freeway, we would detect a lot of      CNN features (19, 21). Dubey et al. (21) performs this anal-\ncars. If the neighborhood is one with many cars parked on the     ysis at a large scale. In these works, a location’s perceived\nstreet, our method would take this into account. On the other     wealth/safety/location is given a number between 0 and 10\nhand, performing this analysis with DMV data associates the       where 10 corresponds to the wealthiest/safest/most unique\ncars owned by residents of a particular neighborhood with the     location.\ndemographic makeup of that neighborhood. Our work also                While the goal of all of these works is to predict people’s\npresents a pipeline to measure various attributes using publicly  perception of a location’s wealth, uniqueness and safety using\navailable visual data. If one were to, for example, study the     images, our goal is different. Our goal is to predict the actual\nrelationship between tree species in a neighborhood and the       median household income, racial makeup, education level, vot-\nhealth of its inhabitants, they can use our methodology (data     ing patterns of a certain location given its street view images.\ncollection, detection, classification etc...) to perform their    Thus, instead of inferring the perception of a particular neigh-\nstudy.                                                            borhood’s safety, uniqueness and wealth given its photos, we\n                                                                  are interested in predicting its true characteristics as recorded\nSatellite Night Lights. Many works, e.g. (10–13), have studied    by government agencies such as the ACS and voter polling\nthe relationship between nighttime lights observed through        stations.\nsatellite imagery and total population, population density,           Arietta et al (22) uses features from convolutional neural\nGDP and a few other variables such as the number and density      networks and other global image features (HOG and GIST) to\nof establishments. While most of these works have focused on      predict housing prices and violent crime rates in San Francisco,\nusing night lights to predict population density and income       Chicago, Boston, Oakland, Seattle and Philadelphia. While\nlevels in developing countries with very little census data, (11) they obtain high correlations between predicted and ground\ninvestigates the use of this technique to estimate income in      truth values when locations in the same city are used for\nnations such as Sweden with near uniform distribution of          training and testing purposes, the accuracy is low while using\nelectricity among its population.                                 different cities for training and testing. For example, while high\n    What (10) found was that in countries such as Sweden where    Pearson r values are achieved between actual and predicted\nliving standards are much more uniform than the developing        housing prices when the same city is used for training and\nworld, the correlation between night time luminosity and total    testing (e.g. 0.815 for Boston), the Pearson r for training and\nwage values was not as high. Using a sophisticated statistical    testing across different cities is much lower (e.g. 0.444 while\nmodel (rather than simple linear regression used in our paper),   training on Boston and testing on Seattle). Thus, these visual\nthe correlation reported by (10) between actual and predicted     attributes do not necessarily generalize across cities.\ntotal wages was 0.52.\n    While, in the developing world, the presence/absence of       10. Baselines\nlights in one’s household is a strong indicator of income levels, In this section, we compare our approach to a number of\nthis is not necessarily the case in countries like the United     baselines that infer demographics from various global image\nStates where almost all citizens have access to electricity.      features and course grained census data.\nAs (13) notes, in the United States where living standards\nare much more uniform than the developing world, the higher       Projecting Course Census Data to Fine Geographic Locations. Here,\nconcentration of lights in coastal areas near the oceans and      we investigate the predictive power of features derived from\nthe Great Lakes reflects the higher population densities there    ACS data at course spatial granularity, to infer demographics\nas opposed to higher income.                                      at finer spatial granularity. Specifically, we train a regression\n    To our knowledge, there is no prior work attempting to        model to infer demographics at the zip code level, using ACS\ninfer race, voting affiliations of education levels purely using  data at the city level. In order to have a baseline that performs\nnight time data. This is because even the correlation between     better than simply assigning all zip codes in a city to the\neconomic output and night time data has been found to be          demographic data of the city, one must assume that some\nrelatively weak in the developed world (10, 11). While certain    demographic data is available at the zip code level. In our\nraces and political affiliations have shown to have historical    case, we assume access to the total population of each zip code.\npreferences for different car brands (14–17), the relationship    Thus, our model is trained using 9 city level and 1 zip code\nbetween night lights and these variables in the United States     level ACS data. That is, each zip code is now represented by\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                            7 of 58\n\na 10 dimensional vector consisting of 9 demographic variables     of training data. We plot the Pearson r between actual and\nat the city level and the number of inhabitants in the zip code.  inferred values using car based and pretrained CNN based\nWe then use the exact same procedure as our methodology           features. The CNN based features have no predictive power\ndescribed in the main text to train a model inferring zip code    with little training data while the car based features have some\nlevel demographic variables. The only difference is that we       (albeit little) predictive power even while using 10% of the data\nuse these 10 ACS variables as our features instead of an 88       for training amounting to just 12 zip codes. However, CNN\ndimensional vector representing each zip code’s car related       based features approach the performance of our car based\nattributes. This baseline was performed using all of the zip      method when large amounts of training data (i.e. over 50%)\ncodes in our data.                                                are available.\n    Figs. S28— S29 plots our results (actual vs inferred values).     We hypothesize that this is due to the low generalizability\nThe Pearson r values and p values for all variables are listed on of Google Street View CNN features to new locations (as\nthe plots. They are r = 0.05, p = 0.007 for median household      mentioned in the related works section above). While using\nincome; r = 0.174, p \034 1e − 7 for the percentage of people        small amounts of training data, no CNN features from locations\nwith less than a high school education; r = 0.1p \034 1e − 7         in visually similar zip codes or cities are available. However, by\nfor the percentage of people with a highschool education;         the time we use more than 50% of our data for training, there\nr = 0.08, p = 7e − 5 for the percentage of people with some       are images from nearby zip codes in the training set, which\ncollege education; r = 0.12, p \034 1e − 7 for the percentage of     ensures high visual similarity between images in the train and\npeople with a bachelor’s degree; r = 0.19, p \034 1e − 7 for the     test set. These experiments show that car based features are\npercentage of people with a graduate degree; r = 0.09, p = 0.1    more predictive and generalizable when little training data is\nfor the percentage of Whites; r = 0.04, p = 0.04 for the          available (such as in our case when we would like to minimize\npercentage of Blacks; r = 0.08, p = 9.8e − 5 for the percentage   the amount of ACS ground truth data required to train a good\nof Asians. Given these results, coarse level census values seem   model). However, it is clear that CNN features have predictive\nto have very little predictive power for inferring education      power and would probably enhance our model’s performance\nlevels, and no ability to infer income or race at a more granular if our method is augmented to include them. Nevertheless,\nlevel.                                                            our simple model trained using car features is much more\n                                                                  interpretable than that trained using 4096 dimensional f c6\nPretrained Convolutional Neural Network Features. Here, we infer  activations. One can simply look at car attributes where our\ndemographics with the same methodology as before but con-         model places high weights, learn the relationship between\nstruct our model using pretrained CNN features instead of car     various demographic variables and car attributes and gain\nattributes detected in Google Street View images. That is,        further insight into American culture.\nour zip code level features consist of f c6 activations from an\nAlexNet (23) CNN instead of the 88 car attributes we used\n                                                                  11. Timelapse Experiments–Inferring Demographics\nbefore. As first shown by (24) in 2014, features from a CNN\n                                                                       Across Time\npretrained on ImageNet have been proven to be much more\ndiscriminative than handcrafted ones such as SIFT, GIST or        Our work so far has used ground truth data from one geo-\nHOG (24).                                                         graphic location to infer demographics in another location\n    We represent each zip code as a 4096 dimensional vector       at the same time point. In this section, we perform prelim-\nwhich is the average of f c6 activations obtained from all images inary experiments to test the feasibility of inferring future\nin a particular zip code. That is, we input each Google Street    demographic trends in a particular neighborhood given its\nView image in a particular zip code to a CNN pretrained on        current and past ACS data. To perform our experiments,\nImageNet. We then take the 4096 f c6 activations from each        we use the recently introduced Google Street View timelapse\nimage and average those in the same zip code to obtain a          tool which shows time-lapse images of the same location over\nsingle feature representation for each geographic location of     time. Fig. S30 shows the dramatic economic development of a\ninterest. We subsequently use the same methodology (ridge         particular address in Brooklyn, New York over time.\nregression) to train a model inferring race, education and            We retrieved Google Street View timelapse images for New\nincome levels obtained from the ACS. We do this analysis          York city using the same methodology to gather Street View\non all images from states that start with “A” in our data         images discussed before. Figs. S31— S32 show maps of the\n(i.e., cities in Alabama, Alaska, Arizona, Arkansas). This        coverage at the district level for each year. Fig. S33 plots\nconsists of the cities: Birmingham (Alabama), Montgomery          the number of images per year for each district. As these\n(Alabama), Anchorage (Alaska), Fairbankds (Alaska), Phoenix       maps and plots show, the number of images is nonuniform\n(Arizona), Tucson (Arizona), Little Rock (Arkansas) and Fort      across years and districts. For example, there is no Statin\nSmith (Arkansas). This comprises of 5,144,334 images which        Island coverage for 2014. This nonuniformity in sampling can\nis slightly higher than 10% of the 50 million images in our data. cause errors in our estimations. Nevertheless, we conducted\nWe performed this analysis on a subset of our dataset due         preliminary experiments to asses the feasibility of performing\nto the large amount of time it takes to obtain CNN features       future research in this area.\nusing our GPU cluster.                                                Once we gathered the images across time, we detected\n    We compare our methodology to this baseline while using       and classified all the cars in these images following the same\na randomly selected subset of 10, 20, 30, 50 and 80 percent       procedure as our retrieved Street View images. For each year,\nof training data, and testing on the rest of the data. This       we represented each district as a collection of the car features\nallows us to investigate how the two approaches compare as we     in that district using the methodology described in the main\nhave access to little or plentiful training data. Figs. S26— S27  text.\nshow how the two methods compare with increasing amounts              We retrieved 2011, 2012, 2013 and 2014 yearly ACS data for\n8 of 58                   Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\neach of the 13 congressional districts in New York city. Yearly   12. Sources of Error\nACS data is only available at the district and county levels\n                                                                  One source of error lies in the quality of the Street View images.\nand for years after 2011. Figs. S34— S35 plots the data of\n                                                                  Images from Street View may have image stitching artifacts,\ninterest (race, education, income) across time for each district.\n                                                                  have street names in the images, and cars in Street View\nThere is very little change from year to year.\n                                                                  images might not be entirely visible, either due to occlusions\n   We used past ACS data and images to train a model predict-     with other objects or simply being cut off by the edge of the\ning future ACS data. Specifically, we trained a ridge regression  image. These factors make car detection and classification\nmodel using 2013 ACS data as ground truth and tested the          in Street View images more difficult. However, images used\nmodel on timelapse images from 2014. Figs. S36— S38 plots         in our performance analysis are also subject to these issues,\nour results. We achieve a very high correlation between ground    and thus our demonstrated performance holds despite these\ntruth results and our predictions (e.g. Pearson r =0.93 for       challenges.\nthe percentage of Asians. All the Pearson coefficients and p          Another source of error consists of biases in sampling, con-\nvalues are listed on the plots). However, since we only have      sisting of either sampling at certain roads or regions preferen-\ndata at the district level and the change from year to year is    tially (due to having incomplete or out of date information\nvery small, a baseline assuming constant ACS data (i.e. no        about roads in cities) or sampling images taken at different\nchange from the prior year) achieves even higher Pearson r        dates. Although being able to properly account for these types\n(r=0.99 for the percentage of Asians). We could not perform       of errors would undoubtedly improve and strengthen the anal-\nthis experiment at the zip code or precinct level for lack of     ysis, these factors do not diminish or weaken the results we\ndata. The ACS does not have yearly data at the zip code level.    already have, and can be considered a source of noise in the\nWe hypothesize that more demographic changes occur at the         data. For example, addressing these limitations might result\nzip code level from year to year. Then, district level ACS data   in stronger correlations and lower p-values. But unless these\ncould be used to calibrate car preferences over time and infer    are systematic errors across the entire United States, they do\nACS data at the zip code level.                                   not affect the validity of the results presented in this work.\n                                                                      Errors in detection and classification of cars can also con-\n   In our second experiment, we wanted to specifically predict    tribute to inacurraccies in the final results. Our car detection\nthe change in ACS data. To do this, we trained a ridge            system does not have perfect precision and recall. Similarly,\nregression model using the difference in ACS data between         the accuracy with which our car classifies all 2,657 classes is\nconsecutive years as ground truth. To encode the change in        not the same. However, we aggregate these classes by make,\ndetected cars, we subtracted the car features for consecutive     body type and other metadata before performing demographic\nyears in each district. These features were used to train the     inference. As shown in Fig. S9, the errors we make at levels\nregression model. We show some preliminary results in Fig. S39    such as the price and body type are reasonable. I.e., we rarely\nshowing the change in the number of high school educated          mistake a very expensive car for a very cheap one, or vice versa.\npeople in NYC (one of the variables that showed some change       This alleviates the level of systematic error in our demographic\nover the years). While the potential to detect demographic        predictions.\ntrends is present, our ability to detect small changes at the         The data used for demographic, crime, and voting analysis\ndistrict level is not currently strong. And more research and     were not collected at the same point in time as images taken\nexperimentation is necessary to have conclusive results.          in Street View, and thus any drift in those sources over time is\n                                                                  also a potential source of error. For example, 2008 presidential\n   To examine the stability of correlations between car types\n                                                                  election data was used in our analyses, but the majority of\nand neighborhood inhabitants, we examine the correlation\n                                                                  Street View images were taken after 2008. This is due to the\nbetween various car attributes and ACS data over time. In\n                                                                  fact that precinct level election data for 2012 was not available\nFig. S40, we plot the correlation between median household\n                                                                  for all of our 200 cities. While this is an unrecoverable source\nincome and various car attributes over time, as well as the\n                                                                  of error, it is primarily a problem when such statistics change\nassociated p values. We see that higher level attributes (such\n                                                                  rapidly over time, and when demographic projections are\nas adjusted car price and age) consistently have a statistically\n                                                                  performed across time. I.e., when inferences for future years\nsignificant positive/negative correlations across time. Some\n                                                                  are performed based on ground truth data for current and\nmakes (e.g. Lamborghinis) also have a consistently statisti-\n                                                                  prior years. Thus, our preliminary timelapse experiments do\ncally significant high correlation with income. E.g. r > 0.65\n                                                                  not use voting data and make inferences based on images and\nfor 2012, 2013, 2014). On the other hand, Porsches have a\n                                                                  ACS data from the appropriate years.\nstatistically significantly high positive correlation with income\nin 2013 and 2014 (r > 0.65) but that correlation is 0.2 for        1. Gebru T, Krause J, Deng J, Fei-Fei L (in press 2017) Scalable annotation of fine-grained\n2012 with a p value of 0.5. The latter is not a statistically         categories without experts in CHI.\n                                                                   2. Deng J, et al. (2009) Imagenet: A large-scale hierarchical image database in Computer Vision\nsignificant correlation. This discrepancy is most probably due        and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. (IEEE), pp. 248–255.\nto errors introduced by the non uniform sampling of images         3. Sheng VS, Provost F, Ipeirotis PG (2008) Get another label? improving data quality and data\n                                                                      mining using multiple, noisy labelers in Proceedings of the 14th ACM SIGKDD international\nacross years and neighborhoods. However, it is still important\n                                                                      conference on Knowledge discovery and data mining. (ACM), pp. 614–622.\nto note that car preferences in a particular location could        4. Su H, Deng J, Fei-Fei L (2012) Crowdsourcing annotations for visual object detection in Work-\nchange over time. To study this effect, these analyses should         shops at the Twenty-Sixth AAAI Conference on Artificial Intelligence.\n                                                                   5. (2012) TIGER/Line - Geography - U.S. Census Bureau (https://www.census.gov/geo/\nbe repeated using images from GPS points that are evenly              maps-data/data/tiger-line.html). Accessed: 2014-11.\nsampled across New York to minimize errors. Nevertheless,          6. (2012) American Community Survey 5 Year Data (2008-2012) (http://www.census.gov/data/\nthese preliminary results show that socioeconomic trends could        developers/data-sets/acs-survey-5-year-data.html). Accessed: 2014-9.\n                                                                   7. Ansolabehere S, Palmer M, Lee A (2014) Precinct-Level Election Data (http://hdl.handle.net/\npotentially be inferred using images to capture the change in         1903.1/21919). Accessed: 2015-1.\ncar types across neighborhoods.                                    8. (year?) (http://www.37billionmilechallenge.org/). Accessed: 2014-10.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                       9 of 58\n\n 9. Cain T (year?) U.S. Auto Sales By Brand - 2011 Year End (http://www.goodcarbadcar.net/\n    2012/01/us-auto-sales-by-brand-2011-year-end.html/).\n10. Mellander C, Lobo J, Stolarick K, Matheson Z (2015) Night-time light data: A good proxy\n    measure for economic activity? PloS one 10(10):e0139779.\n11. Chen X, Nordhaus WD (2011) Using luminosity data as a proxy for economic statistics. Pro-\n    ceedings of the National Academy of Sciences 108(21):8589–8594.\n12. Elvidge CD, et al. (1997) Relation between satellite observed visible-near infrared emissions,\n    population, economic activity and electric power consumption. International Journal of Re-\n    mote Sensing 18(6):1373–1379.\n13. Henderson JV, Storeygard A, Weil DN (2012) Measuring economic growth from outer space.\n    The American Economic Review 102(2):994–1028.\n14. (year?) Differences in new vehicle buyers’ ethnicities predict where future sales will grow\n    (http://www.strategicvision.com/auto_2014_ethnic_release.php).\n15. Myers M, Dean SG (2007) Cadillac flambé”: Race and brand identity in Proceedings of the\n    23rd conference on historical analysis and research in marketing. Vol. 13, pp. 158–161.\n16. Tierney J (2005) Your car: politics on wheels. New York Times 1.\n17. Florida R, Mellander C (2012) Democrat vs. Republican: Who’s Buying What Car?, (Strategic\n    Vision), Technical report.\n18. Salesses P, Schechtner K, Hidalgo CA (2013) The collaborative image of the city: mapping\n    the inequality of urban perception. PloS one 8(7):e68400.\n19. Ordonez V, Berg TL (2014) Learning high-level judgments of urban perception in European\n    Conference on Computer Vision. (Springer), pp. 494–510.\n20. Naik N, Philipoom J, Raskar R, Hidalgo C (2014) Streetscore–predicting the perceived safety\n    of one million streetscapes in 2014 IEEE Conference on Computer Vision and Pattern Recog-\n    nition Workshops. (IEEE), pp. 793–799.\n21. Dubey A, Naik N, Parikh D, Raskar R, Hidalgo CA (2016) Deep learning the city: Quantifying\n    urban perception at a global scale in European Conference on Computer Vision. (Springer),\n    pp. 196–212.\n22. Arietta SM, Efros AA, Ramamoorthi R, Agrawala M (2014) City forensics: Using visual ele-\n    ments to predict non-visual city attributes. IEEE transactions on visualization and computer\n    graphics 20(12):2624–2633.\n23. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional\n    neural networks in Advances in neural information processing systems. pp. 1097–1105.\n24. Donahue J, et al. (2014) Decaf: A deep convolutional activation feature for generic visual\n    recognition. in Icml. Vol. 32, pp. 647–655.\n10 of 58                             Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n   a                                                               b                                                        c\nFig. S1. Confusion matrices show the accuracy with which we classify various car attributes such as type of vehicle in a, whether or not it is domestic in b, and its price in c.\nThe entry in row i Confusion      matrixes\n                   and column j indicates      show the\n                                          the percentage     accuracy\n                                                         of times           with\n                                                                   ground truth    which\n                                                                                attribute j was  classify\n                                                                                             weclassified    various\n                                                                                                          as attribute    car attributes.\n                                                                                                                       i. Thus, the values for all rows in a single column should\nadd up to 1.\n                                  Fig. S2. Bar plots showing the top 10 car features with high positive weight in our race estimation model.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                       11 of 58\n\n   Fig. S3. Scatter plots of ground truth income and race values vs our estimations. Also shown on each plot is the line y =x which corresponds to a perfect predictor.\n12 of 58                       Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n Fig. S4. Scatter plots of ground truth data vs our estimations of educational attainment. Also shown on each plot is the line y =x which corresponds to a perfect predictor.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                  13 of 58\n\nFig. S5. Scatter plots of ground truth data showing the percentage of people with a graduate school degree vs our estimations, and the percentage of people who voted for\nBarack Obama in the 2008 presidential election vs our estimations. Also shown on each plot is the line y =x which corresponds to a perfect predictor.\n14 of 58                        Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n                                                                                                                                                                                                                                                                                                                                                11.5\n                                                                                                                                                                                                                                                                  200\n                                                                                                                                                                                                                                                                                                                                                11\n                                                                                                                                                                                                                                                                  250\n                                                                                                                                                                                                                                                                                                                                                10.5\n                                                                                                                                                                                                                                                                  300\n                                                                                                                                                                                                                                                                                                                                                10\n                                                                                                                                                                                                                                                                  350\n                                                                                                                                                                                                                                                                                                                                                9.5\n                                                                                                                                                                                                                                                                  400\n                                                                                                                                                                                                                                                                                                                                                9\n                                                                                                                                                                                                                                                                  450\n                                                                                                                                                                                                                                                                                                                                                8.5\n                                                                                                                                                                                                                                                                  500\n                                                                                                                                                                                                                                                                                                                                                8\n                                                                                                                                                                                                                                                                         100     200    300   400    500    600          700            800\nA                                                                                                                                   B                                        A                                                                                    C                                 B\n                                          Number of Cars in Street View Images\n                120k                                                                                                                                       Bounding Box Position vs log(area)                                                log(area)                          Bounding Box Position vs Frequency                       Frequency\n                                                                                                                                    50                                                                                                             13\n                                                                                                                                                                                                                                                                                                                                               12000\n                                                                                                                                                                                                                                                                   50\n                100k                                                                                                                                                                                                                               12.5\n                                                                                                                               100\n                                                                                                                                                                                                                                                                  100\n                                                                                                                                                                                                                                                   12                                                                                          10000\n                                                                                                                               150\n                                                                                                                                                                                                                                                                  150\n                        80k                                                                                                                                                                                                                        11.5\n                                                                                                                               200\n                                                                                                                                                                                                                                                                  200\nNum. Images\n                                                                                                                                                                                                                                                   11                                                                                          8000\n                                                                                                                               250                                                                                                                                250\n                        60k                                                                                                                                                                                                                        10.5\n                                                                                                                               300                                                                                                                                300                                                                          6000\n                                                                                                                                                                                                                                                   10\n                                                                                                                               350                                                                                                                                350\n                        40k\n                                                                                                                                                                                                                                                   9.5                                                                                         4000\n                                                                                                                               400                                                                                                                                400\n                                                                                                                                                                                                                                                   9              450\n                        20k                                                                                                    450\n                                                                                                                                                                                                                                                   8.5                                                                                         2000\n                                                                                                                                                                                                                                                                  500\n                                                                                                                               500\n                                                                                                                                                                                                                                                   8              550\n                         0k\n                                  1   2    3    4   5    6     7   8                               9   10                                           100    200              300   400        500               600         700         800                                100     200   300   400   500    600     700            800\n                                                 Num. Bounding Boxes\n                                                                                          B\n                                                                                                                    Figure S8: (A) Bounding box position vs log(area). Each point corresponds to a single bounding\n                                                                      Bounding Box Position vs Frequency     Frequency in our training set of Street View images, and the color corresponds to the log of the number\nFig. S6. (A) Histogram of the number of cars annotated in the Street View images, represented by the number ofboxannotated            bounding boxes in each image. Images included in\n                                                                                                                    of pixels in the bounding box. (B) Bounding box position vs frequency. The color of each pixel\nthese numbers are those images annotated as containing     50 more than zero and less than 11 cars. (B) Bounding box position vs log(area). Each point corresponds to a single\n                                                                                                                   12000\n                                                                                                                    indicates the number of bounding boxes in the training set which overlap that pixel.\nbounding box in our training set of Street View images, and\n                                                          100 the color corresponds to the log of the number of pixels in the bounding box. (C) Bounding box position vs frequency.\nThe color of each pixel indicates the number of bounding150boxes in the training set which overlap with that pixel.10000\n                                                                                                                               200\n                                                                                                                                                                                                                                                  8000\n                                                                                                                                                                                                                                                                                                    33\n                                                                                                                               250\n                                                                                                                               300                                                                                                                6000\n                                                                                                                               350\n                                                                                                                               400                                                                                                                4000\n                          A                                                    Probability Calibration\n                                                                                                     500\n                                                                                                                               450\n                                                                                                                                                                                                                    B                             2000            Detection Precision/Recall\n                              1                                                                                                550\n                                                                                                                                                                                                                    1\n                                                                                                                                                     100    200             300   400       500               600        700     800\n                        0.9                                                                                                                                                                                   0.9\n                                                                                                                   Figure S8: (A) Bounding box position vs log(area). Each point corresponds to a single bounding\n                                                                                                                   box in our training set of Street View images, and the color corresponds to the log of the number\n                        0.8                                                                                        of pixels in the bounding box. (B) Bounding box position 0.8vs frequency. The color of each pixel\n                                                                                                                   indicates the number of bounding boxes in the training set which overlap that pixel.\n                        0.7                                                                                                                                                                                   0.7\nEstimated Probability\n                        0.6                                                                                                                                                                                   0.6\n                                                                                                                                                                                                  Precision\n                                                                                                                                                                                        33\n                        0.5                                                                                                                                                                                   0.5\n                        0.4                                                                                                                                                                                   0.4\n                        0.3                                                                                                                                                                                   0.3\n                        0.2                                                                                                                                                                                   0.2\n                        0.1                                                                                                                                                                                   0.1\n                          0                                                                                                                                                                                         0\n                          −4                   −3                      −2                    −1                                      0                            1                     2                            0           0.1             0.2               0.3          0.4   0.5           0.6          0.7              0.8          0.9\n                                                                                        Detection score                                                                                                                                                                           Recall\nFig. S7. A. The transformation from detection scores to the probability of the detection being correct (i.e. probability of correctly detecting a car), learned with isotonic regression\non the validation set. B. Precision/recall curve for our final detection model on the test set.\n                                                                            0.0470297\n                                                                                                                                                                                                                                                                                                                       0.532939\n                                                                                                                         0.450237                                                                                                                      0.632759\n                                                                                                                                         0.632759\n                                                     0.394558               0.632759    0.236353                                                                 0.762575\n                                                                0.0954128\n                                                                                                       0.0470297\nFig. S8. Example detections with our model on our testing set. Shown in the box around each detection is our estimated probability of the detection having intersection over\nunion greater than 0.5, i.e. counted as correct during detection evaluation.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                                                                                                                                                                                   15 of 58\n\n  A                                                                                                             B\n  C                                                                                                             D\nFig. S9. Confusion matricies of predictions. The entry in row i and column j indicates how many times ground truth attribute i was classified as attribute j. The attributes are A.\nthe make of the car, B. the manufacturing country of the car, C. the model of the car, and D. the body type of the car.\n16 of 58                        Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S10. The Amazon Mechanical Turk (AMT) user interface for grouping visually indistinguishable pairs of classes. The user is asked whether or not the two cars are visually\ndistinct with an option to view more detailed instructions.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                 17 of 58\n\n                        Group 1999                                                                                   Group 3749\nFig. S11. Two examples of classes and the different types of visually indistinguishable cars in each class. Each column is a unique class. The first column shows cars\nassembled into group 1999 whereas the second column shows those in group 3749.\n                    Bad:                                                    Bad:\n           Closeup                                                     Interior                                                       Good\nFig. S12. Left, Middle: Examples of product shot images unsuitable for our dataset, as they are either extremely close up (left) or are of the interior of the car (middle). In order\nto be suitable for recognition, an image must be of the exterior of the car and the car must be entirely visible (right).\n18 of 58                          Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S13. Screenshot of the user interface for labeling images containing a car viewed from the exterior, deployed on Amazon Mechanical Turk. Below the instructions are a set\nof images, and the user is tasked with clicking on the images containing a single prominent vehicle, viewed from the outside. Images the user clicks are moved to the panel on\nthe right side of the screen, and clicks can be undone by clicking on the image in the right panel.\n                                Raw                                                                          Unwarped\nFig. S14. An example of the unwarping that needs to be done on images retrieved from Street View. Left: an image from Street View as initially scraped. The image appears\nwarped (e.g. straight lines in the real world are not straight in the image) due to the equirectangular projection used to store spherical panoramas. Right: the result of undoing\nthis projection, which we do before using the images any further.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                      19 of 58\n\n                                       A                                                                                         B\n                                                                                   C\nFig. S15. Screenshots of the user interface for hierarchically annotating Street View images with car categories. A. The expert is first asked to identify the make. B. The next\nstep in the task is to identify the body type of the car which is called submodel in the task. C. Once the body type is identified we provide a list of classes for the selected make\nand body type. Example images of each class are also shown to aid the user in identification.\n20 of 58                           Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n                                                              Distribution of %Obama\n               1200\n               1000\n               800\n       Count   600\n               400\n               200\n                    0\n                     0                         0.2                        0.4                        0.6                        0.8                          1\n                                                                               %Obama\n        Fig. S16. Histogram of the fraction of votes cast for Barack Obama vs. John McCain in the 2008 presidential election for the precincts in our dataset.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                21 of 58\n\nFig. S17. Scatter plots of cross validated actual versus predicted education and median household income levels. Also shown on each plot is the line y = x, which corresponds\nto a perfect predictor.\n22 of 58                         Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S18. Scatter plots of crossvalidated actual versus predicted distributions of race. Also shown on each plot is the line y = x, which corresponds to a perfect predictor. The\nlast scatter plot shows cross validated actual vs predicted voting results.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                     23 of 58\n\n                               (a) Average car price                                                              (b) Percentage of foreign cars\n                             (c) Percentage of BMWs                                                                (d) Percentage of Chevrolets\n                          (e) Percentage of Toyota Prius                                                           (f) Percentage of Ford F-150\n      Fig. S19. Maps of a variety of car attributes as measured across the cities in our dataset. Each point corresponds to one city. Not shown: Alaska and Hawaii.\n24 of 58                      Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S20. Our methodology applied to predicting additional ACS attributes not discussed in the main text. Note: this is an application of our methodology to infer variables with\nno refinement. Best viewed after zooming in.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                    25 of 58\n\n                                   1.5k GPS samples            143k GPS samples\n                                                                                                                                                          Chevrolet Silverado 2500 Regular Cab 2000-2002 LS\n                        200 cities, 50M images                                    21.8M DPM car detections\n                                                                                                                                                          Chevrolet Silverado 1500 Extended Cab 2007-2008\n                                                                                                                                                                                 LT1\nC(A)                                                                                                             (B)\n                                                                                                                  D\nPearson r\n                                                                                                                   Percentage of total cars\n                                                                                                                                              Zip codes\n            Fig. S21. A. Correlation between our detected makes and Massachusetts DMV data for 30 makes. B. The percentage of registered Hondas vs. Those we detect.\n                                Fig. S22. The percentage of registered makes in each zip code (according to DMV data) vs. those we detect for each make.\n26 of 58                                     Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n       Fig. S23. The percentage of registered makes in each zip code (according to DMV data) vs. those we detect for each make (continued from prior page).\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                         27 of 58\n\n                                                                                           A\n            0.9\n            0.8\n            0.6\nPearson r   0.4\n            0.2\n         0.0\n        -0.1\n                                                                                    Makes\n                                                                                       B\n            1.0\n            0.8\nPearson r\n            0.6\n            0.4\n            0.2\n            0.0\n                                                                               Zip Codes\nFig. S24. A. The correlation between the distribution of detected and registered car makes across zip codes in the three cities in Massachusetts, Boston, Springfield, and\nWorcester. We show results for all 45 makes in the intersection of our and DMV data. B. The correlation between the distribution of detected and registered car makes in each\nzip code for the three cities in Massachusetts (Boston, Springfield, Worcester). All zip codes have correlation greater than 0.8.\n28 of 58                       Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n                                                                                    A\n                                                                                    B\nFig. S25. (A) The Distribution of registered makes in Boston, Springfield, and Worcester Massachusetts. (B) The distribution of detected makes in Boston, Springfield, and\nWorcester Massachusetts.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                              29 of 58\n\nFig. S26. Comparisons of our methodology with a baseline using features from a convolutional neural network pretrained on ImageNet. This experiment was carried out using\n10% of our data (approximately half a million images). The x axis shows the percentage of training data and the y axis shows the Pearson correlation coefficient between\nactual and predicted values.\n30 of 58                      Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S27. Comparisons of our methodology with a baseline using features from a convolutional neural network pretrained on ImageNet. This experiment was carried out using\n10% of our data (approximately half a million images). The x axis shows the percentage of training data and the y axis shows the Pearson correlation coefficient between\nactual and predicted values (continued from prior page).\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                             31 of 58\n\n                                                                                      (b) Percentage of People with Less than a High school Education (Ratios between 0\n                         (a) Median Household Income ($)                              and 1)\n (c) Percentage of People with a High school Education (Ratios between 0 and 1)         (d) Percentage of People with Some College Education (Ratios between 0 and 1)\n   (e) Percentage of People with a Bachelors Degree (Ratios between 0 and 1)             (f) Percentage of People with a Graduate Degree (Ratios between 0 and 1)\n   Fig. S28. Zip code level income, race and education variables from the ACS inferred using city level data for the same variable and population data for each zip code.\n32 of 58                       Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n               (a) Percentage of Whites (Ratios between 0 and 1)                                        (b) Percentage of Blacks (Ratios between 0 and 1)\n               (c) Percentage of Asians (Ratios between 0 and 1)\n  Fig. S29. Zip code level income, race and education variables from the ACS inferred using city level data for the same variable and population data for each zip code.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                33 of 58\n\n                                                Google Time Lapse\n                                2007                                                                             2014\n                                                       New York City, New York\nFig. S30. Google Street View Timelapse Images of a particular Neighborhood in Brooklyn New York. The economic development of this neighborhood is apparent from its\ntimelapse images in 2007 and 2014 depicting its transformation.\n34 of 58                      Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n                                  (a) 2007                                                                         (b) 2008\n                                  (c) 2009                                                                         (d) 2010\n    Fig. S31. Maps showing a random sample of GPS points where Google Street View timelapse images were retrieved for New York city between 2007 and 2014.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                     35 of 58\n\n                                 (a) 2011                                                                           (b) 2012\n                                 (c) 2013                                                                        (d) 2014\n    Fig. S32. Maps showing a random sample of GPS points where Google Street View timelapse images were retrieved for New York city between 2007 and 2014.\n36 of 58                   Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n           Fig. S33. Bar plots depicting the number of Google Street View timelapse images retrieved for each congressional district in New York city.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                      37 of 58\n\nFig. S34. Plots showing the change in various ACS socioeconomic variables from 2011—2014 for each of the 13 congressional districts in New York city. There is very little\nchange at the district level.\n38 of 58                      Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S35. Plots showing the change in various ACS socioeconomic variables from 2011—2014 for each of the 13 congressional districts in New York city. There is very little\nchange at the district level (continued from prior page).\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                            39 of 58\n\nFig. S36. Results for inferring 2014 demographic variables in New York City’s congressional districts using 2013 ACS data for New York city and Google Street View timelapse\nimages for 2014. Left column shows the results applying our methodology. Right column shows a baseline assuming now change in demographic variables. Assuming no\nchange gives better results because we are not specifically training a model to predict changes and there is very little change in NYC ACS data from 2013 to 2014 at the district\nlevel.\n40 of 58                         Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S37. (Continued from Prior Page) Results for inferring 2014 demographic variables in New York City’s congressional districts using 2013 ACS data for New York city and\nGoogle Street View timelapse images for 2014. Left column shows the results applying our methodology. Right column shows a baseline assuming now change in demographic\nvariables. Assuming no change gives better results because we are not specifically training a model to predict changes and there is very little change in NYC ACS data from\n2013 to 2014 at the district level.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                41 of 58\n\nFig. S38. (Continued from Prior Page) Results for inferring 2014 demographic variables in New York City’s congressional districts using 2013 ACS data for New York city and\nGoogle Street View timelapse images for 2014. Left column shows the results applying our methodology. Right column shows a baseline assuming now change in demographic\nvariables. Assuming no change gives better results because we are not specifically training a model to predict changes and there is very little change in NYC ACS data from\n2013 to 2014 at the district level.\n42 of 58                         Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\nFig. S39. The change in the percentage of people with less than a high school education in each of the 13 New York city congressional districts between 2013 and 2014. Left is\nthe actual ACS Value and Right is our Prediction. Red signifies a decrease in percentage and blue an increase.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                                                                 43 of 58\n\nFig. S40. The Pearson correlation coefficient between various car attributes detected using our method and median household income across time. The p values are also\nlisted next to each point. This tests the stability of the correlations across time. Note that many errors are most probably introduced due to the nonuniform sampling of GPS\npoints across time. Note: zoom in to see a detailed view of each plot.\n44 of 58                          Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n  City                         # Im.   City                     # Im. City                   # Im. City                      # Im.\n  Birmingham, AL            484,818    Santa Ana, CA          90,030  Portland, ME         86,874  Salem, OR              102,174\n  Huntsville, AL            100,410    Santa Clarita, CA      83,298  Baltimore, MD       570,360  Philadelphia, PA       244,194\n  Mobile, AL                 45,114    Santa Rosa, CA        243,324  Frederick, MD       182,388  Pittsburgh, PA         682,728\n  Montgomery, AL             45,084    Stockton, CA          343,662  Boston, MA          195,864  Providence, RI         130,104\n  Anchorage, AK              59,484    Sunnyvale, CA          66,318  Springfield, MA     116,928  Warwick, RI            172,092\n  Fairbanks, AK              42,384    Torrance, CA          136,260  Worcester, MA       197,424  Charleston, SC          56,604\n  Chandler, AZ              309,414    Aurora, CO            143,508  Detroit, MI         287,736  Columbia, SC           334,914\n  Gilbert, AZ               175,242    Colorado Springs, CO  492,222  Grand Rapids, MI    202,266  Rapid City, SD          30,954\n  Glendale, AZ              160,146    Denver, CO            306,990  Minneapolis, MN     654,270  Sioux Falls, SD         74,640\n  Mesa, AZ                  283,620    Fort Collins, CO      307,056  Saint Paul, MN      164,034  Chattanooga, TN        284,214\n  Peoria, AZ                135,132    Bridgeport, CT        154,092  Gulfport, MS         14,898  Knoxville, TN          457,434\n  Phoenix, AZ               623,892    New Haven, CT          62,394  Jackson, MS          71,298  Memphis, TN             97,572\n  Scottsdale, AZ            138,120    Dover, DE              22,134  Kansas City, MO     577,830  Nashville, TN          554,118\n  Tempe, AZ                 302,958    Wilmington, DE         80,754  Springfield, MO     395,502  Amarillo, TX            85,380\n  Tucson, AZ                634,986    Washington, DC        375,258  St. Louis, MO       426,942  Arlington, TX          509,406\n  Fort Smith, AR            205,512    Cape Coral, FL        309,102  Billings, MT         54,768  Austin, TX             211,530\n  Little Rock, AR           398,094    Fort Lauderdale, FL   279,300  Missoula, MT        157,254  Brownsville, TX        284,826\n  Anaheim, CA               133,098    Hialeah, FL           143,928  Lincoln, NE         444,306  Corpus Christi, TX      61,434\n  Bakersfield, CA           521,112    Jacksonville, FL      770,016  Omaha, NE           322,602  Dallas, TX             663,006\n  Chula Vista, CA           189,204    Miami, FL             310,692  Henderson, NV       259,416  El Paso, TX            205,500\n  Corona, CA                238,932    Orlando, FL           582,018  Las Vegas, NV       521,172  Fort Worth, TX         677,214\n  Elk Grove, CA             306,600    Pembroke Pines, FL     71,274  North Las Vegas, NV 197,394  Garland, TX            226,140\n  Escondido, CA             206,550    Port St. Lucie, FL     62,292  Reno, NV            104,328  Grand Prairie, TX      210,198\n  Fontana, CA               167,604    Saint Petersburg, FL   83,442  Manchester, NH      131,682  Houston, TX            337,830\n  Fremont, CA               232,608    Tallahassee, FL       419,220  Nashua, NH          139,890  Irving, TX             179,382\n  Fresno, CA                135,210    Tampa, FL             610,770  Jersey City, NJ      78,036  Laredo, TX             259,878\n  Garden Grove, CA           77,706    Atlanta, GA           315,336  Newark, NJ          129,948  Lubbock, TX            500,760\n  Glendale, CA               77,316    Augusta, GA           239,994  Albuquerque, NM      73,746  Pasadena, TX            29,700\n  Hayward, CA               207,744    Columbus, GA           54,246  Las Cruces, NM       82,098  Plano, TX              330,186\n  Huntington Beach, CA      101,574    Hilo, HI               14,406  Buffalo, NY         376,806  San Antonio, TX      1,034,358\n  Irvine, CA                183,474    Honolulu, HI          209,010  New York, NY        508,860  Salt Lake City, UT     272,190\n  Lancaster, CA             110,550    Boise, ID              42,438  Rochester, NY       391,458  West Valley City, UT    69,432\n  Long Beach, CA            265,806    Nampa, ID             231,318  Yonkers, NY          27,618  Burlington, VT          31,998\n  Los Angeles, CA           554,106    Aurora, IL            203,256  Charlotte, NC       111,510  Essex, VT               16,056\n  Modesto, CA                32,406    Chicago, IL           791,298  Durham, NC          359,592  Alexandria, VA          69,924\n  Moreno Valley, CA         180,516    Joliet, IL            118,116  Fayetteville, NC    292,296  Chesapeake, VA          38,568\n  Oakland, CA               326,208    Rockford, IL          372,156  Greensboro, NC       80,730  Newport News, VA        17,862\n  Oceanside, CA             129,384    Fort Wayne, IN         99,672  Raleigh, NC         409,776  Norfolk, VA             56,688\n  Ontario, CA               142,230    Indianapolis, IN      468,780  Winston-Salem, NC   457,314  Richmond, VA           504,138\n  Oxnard, CA                154,074    Cedar Rapids, IA      257,178  Bismarck, ND        156,912  Virginia Beach, VA      40,698\n  Palmdale, CA              164,064    Des Moines, IA        123,678  Fargo, ND           202,422  Seattle, WA            529,392\n  Pomona, CA                153,798    Kansas City, KS       577,830  Akron, OH           404,376  Spokane, WA            381,684\n  Rancho Cucamonga, CA       88,734    Overland Park, KS       9,252  Cincinnati, OH      511,842  Tacoma, WA             331,338\n  Riverside, CA             446,412    Wichita, KS           569,658  Cleveland, OH       416,142  Vancouver, WA          292,560\n  Sacramento, CA            525,756    Lexington, KY         345,516  Columbus, OH        568,776  Charleston, WV          38,628\n  Salinas, CA               175,530    Louisville, KY        419,544  Toledo, OH           51,444  Huntington, WV          42,144\n  San Bernardino, CA        124,002    Baton Rouge, LA        65,592  Oklahoma City, OK   687,234  Madison, WI            218,580\n  San Diego, CA             472,872    New Orleans, LA       456,042  Tulsa, OK           541,458  Milwaukee, WI          446,172\n  San Francisco, CA         215,298    Shreveport, LA        100,662  Eugene, OR          108,582  Casper, WY              43,542\n  San Jose, CA              274,848    Lewiston, ME           50,562  Portland, OR        548,334  Cheyenne, WY           211,668\nTable S1. List of cities we collected Street View images for and the\nnumber of Street View images we collected for each city.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                            45 of 58\n\n    Attribute                        Training   Validation      Test\n    Street View Images               199,666      39,933      159,732\n    Product Shot Images              313,099         -            -\n    Total Images                     512,765      39,933      159,732\n    Street View Bounding Boxes       272,142      54,691      216,808\n    Product Shot Bounding Boxes      313,099         -            -\n    Total Bounding Boxes             585,241      54,691      216,808\n    Street View Category Labels       34,753       6,921      27,888\n    Product Shot Category Labels     313,099         -            -\n    Total Category Labels            347,852       6,921      27,888\nTable S2. Dataset statistics for our training, validation, and test splits,\nseparated into Street View and product shot images.\n                     Comp.     Parts      AP    Time\n                       1         0       52.3    2.27\n                       1         4       63.2    3.48\n                       1         8       64.2    4.84\n                       3         0       62.9    6.48\n                       3         4       66.7   12.20\n                       3         8       68.4   16.47\n                       5         0       64.8   10.25\n                       5         4       67.3   16.33\n                       5         8       68.7   22.07\n                       6         0       65.2   10.48\n                       8         0       66.0   11.17\nTable S3. Average Precision (AP) on the Street View validation set for\nvarious DPM configurations. Time is measured in seconds per image.\nComp. is the number of DPM components, and Parts indicates the\nnumber of parts in the model.\n46 of 58                   Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n  Variable                    Pearson’s r      p-value   Variable             Pearson’s r    p-value\n  Price                           0.4435    1.16e-117    Make:  Fiat              0.0747   0.000226\n  Cars/Image                      0.0235         0.247   Make:  Fisker            0.0751   0.000213\n  MPG Highway                     0.1642      3.72e-16   Make:  Ford             -0.2697   9.17e-42\n  MPG City                        0.2565      8.08e-38   Make:  Geo              -0.2051   1.73e-24\n  Hybrid                          0.1169      7.58e-09   Make:  GMC              -0.1627   7.08e-16\n  Electric                        0.1589      3.35e-15   Make:  Honda             0.4234  2.67e-106\n  Foreign                         0.4672    5.26e-132    Make:  Hummer            0.0799   7.99e-05\n  Country: England                0.3126      3.15e-56   Make:  Hyundai           0.1201   2.91e-09\n  Country: Germany                0.4335      6.3e-112   Make:  Infiniti          0.2649   2.58e-40\n  Country: Italy                  0.2167      3.18e-27   Make:  Isuzu            -0.1252   5.91e-10\n  Country: Japan                  0.4471    1.01e-119    Make:  Jaguar           -0.0397      0.0504\n  Country: South Korea            0.0834      3.83e-05   Make:  Jeep              0.0153       0.452\n  Country: Sweden                 0.2553       1.8e-37   Make:  Kia              -0.0239       0.238\n  Country: USA                   -0.4672    5.26e-132    Make:  Lamborghini       0.1999   2.56e-23\n  Body Type: Convertible          0.1484      1.95e-13   Make:  Land Rover        0.3000       1e-51\n  Body Type: Coupe               -0.2211      2.69e-28   Make:  Lexus             0.4432  1.73e-117\n  Body Type: Crew Cab            -0.0002         0.993   Make:  Lincoln          -0.1652   2.54e-16\n  Body Type: Extended Cab        -0.0943      3.19e-06   Make:  Lotus             0.1232   1.11e-09\n  Body Type: Hatchback            0.3352      7.16e-65   Make:  Maserati          0.1096   6.05e-08\n  Body Type: Minivan              0.0833      3.94e-05   Make:  Maybach           0.0570    0.00494\n  Body Type: Regular Cab         -0.2179       1.7e-27   Make:  Mazda             0.2094   1.76e-25\n  Body Type: Sedan               -0.1537      2.62e-14   Make:  Mclaren           0.1002   7.54e-07\n  Body Type: SUV                  0.3136      1.34e-56   Make:  Mercedes-Benz     0.3873   8.94e-88\n  Body Type: Van                 -0.0391        0.0542   Make:  Mercury          -0.3367   1.71e-65\n  Body Type: Wagon                0.1776      1.14e-18   Make:  Mini              0.2749   2.21e-43\n  Year: 1990-1994                -0.3230      4.21e-60   Make:  Mitsubishi       -0.0739   0.000269\n  Year: 1995-1999                -0.4202      1.5e-104   Make:  Nissan            0.0894   1.01e-05\n  Year: 2000-2004                 0.2043      2.56e-24   Make:  Oldsmobile       -0.3964   3.12e-92\n  Year: 2005-2009                 0.3864      2.38e-87   Make:  Panoz             0.0507      0.0124\n  Year: 2010-2014                 0.3694      2.01e-79   Make:  Plymouth         -0.2496   7.85e-36\n  Make: Acura                     0.3528      3.78e-72   Make:  Pontiac          -0.3805   1.46e-84\n  Make: AM General                0.0008         0.969   Make:  Porsche           0.2967   1.45e-50\n  Make: Aston Martin              0.0934         4e-06   Make:  Ram              -0.0513      0.0114\n  Make: Audi                      0.3420      1.19e-67   Make:  Rolls-Royce       0.0843   3.18e-05\n  Make: Bentley                  -0.0319         0.115   Make:  Saab              0.2215   2.14e-28\n  Make: BMW                       0.3939      5.53e-91   Make:  Saturn           -0.0793   9.11e-05\n  Make: Buick                    -0.3975      8.43e-93   Make:  Scion             0.2463   6.47e-35\n  Make: Cadillac                 -0.3248      8.39e-61   Make:  Smart             0.1464   4.15e-13\n  Make: Chevrolet                -0.3553      3.08e-73   Make:  Subaru            0.1727   1.03e-17\n  Make: Chrysler                 -0.2720      1.81e-42   Make:  Suzuki           -0.0679   0.000817\n  Make: Daewoo                   -0.0214         0.293   Make:  Tesla             0.0860   2.19e-05\n  Make: Dodge                    -0.3807      1.22e-84   Make:  Toyota            0.4239  1.43e-106\n  Make: Eagle                    -0.2009      1.55e-23   Make:  Volkswagen        0.3014   3.24e-52\n  Make: Ferrari                   0.0694     0.000619    Make:  Volvo             0.2398     3.9e-33\nTable S4. Pearson r correlation coefficients and their associated p-\nvalues for each car attribute between median household income and\neach car attribute, at the zip code level. p-values are with respect to\nthe null hypothesis of no correlation.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei    47 of 58\n\n  Variable                  Pearson’s r    p-value   Variable             Pearson’s r   p-value\n  Price                        -0.3333      4e-64    Make:  Fiat             -0.0721  0.000372\n  Cars/Image                    0.1246   7.06e-10    Make:  Fisker           -0.1153  1.19e-08\n  MPG Highway                  -0.2920   5.69e-49    Make:  Ford              0.2991    2.1e-51\n  MPG City                     -0.3182   2.57e-58    Make:  Geo               0.1532  3.18e-14\n  Hybrid                       -0.1331   4.43e-11    Make:  GMC               0.1745  4.48e-18\n  Electric                     -0.1508   7.82e-14    Make:  Honda            -0.2530  8.23e-37\n  Foreign                      -0.2955   3.64e-50    Make:  Hummer            0.0199      0.328\n  Country: England             -0.2434   4.13e-34    Make:  Hyundai          -0.3468  1.26e-69\n  Country: Germany             -0.3029   1.01e-52    Make:  Infiniti         -0.1423    1.8e-12\n  Country: Italy               -0.1587    3.6e-15    Make:  Isuzu             0.1501  1.03e-13\n  Country: Japan               -0.2301   1.45e-30    Make:  Jaguar           -0.0182        0.37\n  Country: South Korea         -0.3456   4.03e-69    Make:  Jeep             -0.2030  5.22e-24\n  Country: Sweden              -0.3541   1.04e-72    Make:  Kia              -0.1942    4.4e-22\n  Country: USA                  0.2955   3.64e-50    Make:  Lamborghini      -0.1403  3.68e-12\n  Body Type: Convertible       -0.2246   3.67e-29    Make:  Land Rover       -0.1698  3.55e-17\n  Body Type: Coupe              0.0892   1.06e-05    Make:  Lexus            -0.2421    9.4e-34\n  Body Type: Crew Cab           0.1080   9.47e-08    Make:  Lincoln           0.1483  2.05e-13\n  Body Type: Extended Cab       0.2204   3.98e-28    Make:  Lotus            -0.0684  0.000745\n  Body Type: Hatchback         -0.3445   1.13e-68    Make:  Maserati         -0.0745  0.000239\n  Body Type: Minivan           -0.0206        0.31   Make:  Maybach          -0.0537   0.00806\n  Body Type: Regular Cab        0.2732   7.55e-43    Make:  Mazda            -0.2595  1.11e-38\n  Body Type: Sedan             -0.0173       0.395   Make:  Mclaren          -0.0764  0.000164\n  Body Type: SUV               -0.2361   3.88e-32    Make:  Mercedes-Benz    -0.1895  4.47e-21\n  Body Type: Van                0.2208   3.32e-28    Make:  Mercury           0.0984  1.16e-06\n  Body Type: Wagon             -0.3888   1.55e-88    Make:  Mini             -0.2471  3.94e-35\n  Year: 1990-1994               0.3230   3.87e-60    Make:  Mitsubishi        0.1061  1.58e-07\n  Year: 1995-1999               0.3715   2.12e-80    Make:  Nissan            0.1217  1.78e-09\n  Year: 2000-2004              -0.1652    2.5e-16    Make:  Oldsmobile        0.1131  2.24e-08\n  Year: 2005-2009              -0.3831   8.51e-86    Make:  Panoz            -0.1770  1.47e-18\n  Year: 2010-2014              -0.3296   1.17e-62    Make:  Plymouth          0.1237    9.6e-10\n  Make: Acura                  -0.1895   4.45e-21    Make:  Pontiac           0.0520     0.0103\n  Make: AM General              0.0676    0.00085    Make:  Porsche          -0.2387  8.08e-33\n  Make: Aston Martin           -0.1157   1.07e-08    Make:  Ram              -0.0817  5.54e-05\n  Make: Audi                   -0.3176   4.47e-58    Make:  Rolls-Royce      -0.1050  2.12e-07\n  Make: Bentley                 0.0356     0.0792    Make:  Saab             -0.2844  1.93e-46\n  Make: BMW                    -0.1956   2.24e-22    Make:  Saturn           -0.1775  1.19e-18\n  Make: Buick                   0.0353     0.0822    Make:  Scion            -0.1481  2.14e-13\n  Make: Cadillac                0.1866   1.81e-20    Make:  Smart            -0.1571  6.68e-15\n  Make: Chevrolet               0.3183   2.46e-58    Make:  Subaru           -0.3597  3.82e-75\n  Make: Chrysler                0.0264       0.194   Make:  Suzuki           -0.0054        0.79\n  Make: Daewoo                  0.0122       0.548   Make:  Tesla            -0.0661   0.00112\n  Make: Dodge                   0.2208   3.24e-28    Make:  Toyota           -0.1686  6.02e-17\n  Make: Eagle                   0.0612    0.00254    Make:  Volkswagen       -0.2975  7.55e-51\n  Make: Ferrari                -0.0584    0.00396    Make:  Volvo            -0.3376  7.58e-66\nTable S5. Pearson r correlation coefficients and their associated p-\nvalues for each car attribute between the percentage of residents\nwho did not graduate high school and each car attribute, at the zip\ncode level.\n48 of 58                  Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n  Variable                  Pearson’s r     p-value    Variable             Pearson’s r    p-value\n  Price                        -0.4345   1.91e-112     Make:  Fiat             -0.0925     4.9e-06\n  Cars/Image                   -0.2684    2.34e-41     Make:  Fisker           -0.1364   1.45e-11\n  MPG Highway                  -0.3294    1.33e-62     Make:  Ford              0.3735   2.78e-81\n  MPG City                     -0.4373   4.66e-114     Make:  Geo               0.1799   4.06e-19\n  Hybrid                       -0.1288    1.88e-10     Make:  GMC               0.2760   1.01e-43\n  Electric                     -0.2963    1.92e-50     Make:  Honda            -0.5371  1.12e-181\n  Foreign                      -0.6048   2.04e-242     Make:  Hummer           -0.0502      0.0134\n  Country: England             -0.4811   5.74e-141     Make:  Hyundai          -0.0521      0.0102\n  Country: Germany             -0.6142   5.35e-252     Make:  Infiniti         -0.3755   3.16e-82\n  Country: Italy               -0.2462    6.92e-35     Make:  Isuzu             0.0364      0.0731\n  Country: Japan               -0.5670   9.84e-207     Make:  Jaguar           -0.0201       0.322\n  Country: South Korea         -0.0235        0.247    Make:  Jeep             -0.0247       0.223\n  Country: Sweden              -0.4033      9.8e-96    Make:  Kia               0.0442      0.0292\n  Country: USA                  0.6048   2.04e-242     Make:  Lamborghini      -0.1902   3.14e-21\n  Body Type: Convertible       -0.2258    1.84e-29     Make:  Land Rover       -0.4198  2.38e-104\n  Body Type: Coupe              0.1453    6.19e-13     Make:  Lexus            -0.4841  5.05e-143\n  Body Type: Crew Cab           0.1141    1.69e-08     Make:  Lincoln           0.2109   7.74e-26\n  Body Type: Extended Cab       0.2023    7.48e-24     Make:  Lotus            -0.0922   5.33e-06\n  Body Type: Hatchback         -0.5576   1.38e-198     Make:  Maserati         -0.1612     1.3e-15\n  Body Type: Minivan            0.0811    6.31e-05     Make:  Maybach          -0.0328       0.106\n  Body Type: Regular Cab        0.3076    2.01e-54     Make:  Mazda            -0.3406   4.81e-67\n  Body Type: Sedan              0.0382       0.0601    Make:  Mclaren          -0.1054   1.89e-07\n  Body Type: SUV               -0.2750    1.97e-43     Make:  Mercedes-Benz    -0.4409  3.81e-116\n  Body Type: Van                0.0753    0.000204     Make:  Mercury           0.3778   2.66e-83\n  Body Type: Wagon             -0.3653    1.41e-77     Make:  Mini             -0.4305  3.09e-110\n  Year: 1990-1994               0.3364    2.25e-65     Make:  Mitsubishi        0.0650    0.00135\n  Year: 1995-1999               0.4220   1.51e-105     Make:  Nissan           -0.1273   3.02e-10\n  Year: 2000-2004              -0.2078    4.08e-25     Make:  Oldsmobile        0.4061   3.97e-97\n  Year: 2005-2009              -0.4040    4.68e-96     Make:  Panoz            -0.0190       0.349\n  Year: 2010-2014              -0.3541    1.06e-72     Make:  Plymouth          0.3036   5.54e-53\n  Make: Acura                  -0.3951      1.3e-91    Make:  Pontiac           0.3608   1.34e-75\n  Make: AM General              0.0806    6.91e-05     Make:  Porsche          -0.3736   2.44e-81\n  Make: Aston Martin           -0.1640    4.18e-16     Make:  Ram               0.1413   2.58e-12\n  Make: Audi                   -0.4816   2.64e-141     Make:  Rolls-Royce      -0.1119   3.19e-08\n  Make: Bentley                 0.0386       0.0569    Make:  Saab             -0.3086   8.85e-55\n  Make: BMW                    -0.5318   1.81e-177     Make:  Saturn            0.0705   0.000509\n  Make: Buick                   0.4488   9.49e-121     Make:  Scion            -0.2673       5e-41\n  Make: Cadillac                0.3722    1.01e-80     Make:  Smart            -0.2644   3.66e-40\n  Make: Chevrolet               0.4747   8.44e-137     Make:  Subaru           -0.3302   6.37e-63\n  Make: Chrysler                0.3358    3.92e-65     Make:  Suzuki           -0.0052         0.8\n  Make: Daewoo                 -0.0043        0.832    Make:  Tesla            -0.1519   5.15e-14\n  Make: Dodge                   0.5527   1.99e-194     Make:  Toyota           -0.5068   9.3e-159\n  Make: Eagle                   0.1834    7.98e-20     Make:  Volkswagen       -0.5290  2.39e-175\n  Make: Ferrari                -0.1523      4.5e-14    Make:  Volvo            -0.3879   4.55e-88\nTable S6. Pearson r correlation coefficients and their associated p-\nvalues for each car attribute between the percentage of residents\nwith a high school degree and each car attribute, at the zip code\nlevel.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei  49 of 58\n\n  Variable                  Pearson’s r     p-value    Variable             Pearson’s r   p-value\n  Price                        -0.2577    3.62e-38     Make:  Fiat             -0.0947  2.92e-06\n  Cars/Image                   -0.4257   1.43e-107     Make:  Fisker           -0.1472  3.03e-13\n  MPG Highway                  -0.3847      1.5e-86    Make:  Ford              0.2781  2.18e-44\n  MPG City                     -0.3933    1.01e-90     Make:  Geo               0.0910      7e-06\n  Hybrid                       -0.1792    5.51e-19     Make:  GMC               0.2901  2.46e-48\n  Electric                     -0.2099    1.35e-25     Make:  Honda            -0.3382  4.53e-66\n  Foreign                      -0.3834    6.37e-86     Make:  Hummer           -0.0548   0.00693\n  Country: England             -0.4480   2.85e-120     Make:  Hyundai          -0.1058  1.72e-07\n  Country: Germany             -0.4791   1.16e-139     Make:  Infiniti         -0.3544  7.84e-73\n  Country: Italy               -0.2939    1.25e-49     Make:  Isuzu             0.0305      0.133\n  Country: Japan               -0.3221    8.96e-60     Make:  Jaguar           -0.2449  1.61e-34\n  Country: South Korea         -0.0805    7.07e-05     Make:  Jeep             -0.0230      0.257\n  Country: Sweden              -0.2889    6.45e-48     Make:  Kia               0.0052      0.799\n  Country: USA                  0.3834    6.37e-86     Make:  Lamborghini      -0.2414  1.48e-33\n  Body Type: Convertible       -0.0883    1.29e-05     Make:  Land Rover       -0.3614    7.2e-76\n  Body Type: Coupe              0.0791    9.54e-05     Make:  Lexus            -0.2781  2.03e-44\n  Body Type: Crew Cab           0.3601    2.67e-75     Make:  Lincoln          -0.0891  1.08e-05\n  Body Type: Extended Cab       0.4391   4.61e-115     Make:  Lotus            -0.1283  2.21e-10\n  Body Type: Hatchback         -0.3453    5.36e-69     Make:  Maserati         -0.1971  1.06e-22\n  Body Type: Minivan           -0.0567     0.00514     Make:  Maybach          -0.0698  0.000579\n  Body Type: Regular Cab        0.4194   3.99e-104     Make:  Mazda            -0.2630  1.02e-39\n  Body Type: Sedan             -0.2780      2.2e-44    Make:  Mclaren          -0.1225  1.37e-09\n  Body Type: SUV               -0.1038    2.89e-07     Make:  Mercedes-Benz    -0.3848  1.29e-86\n  Body Type: Van               -0.1936    5.98e-22     Make:  Mercury           0.0195      0.337\n  Body Type: Wagon             -0.1893    4.75e-21     Make:  Mini             -0.3150  4.19e-57\n  Year: 1990-1994               0.2961      2.2e-50    Make:  Mitsubishi        0.0614   0.00248\n  Year: 1995-1999               0.1242    8.16e-10     Make:  Nissan           -0.1723  1.17e-17\n  Year: 2000-2004              -0.0270        0.183    Make:  Oldsmobile        0.1694  4.19e-17\n  Year: 2005-2009              -0.2729    9.06e-43     Make:  Panoz            -0.0402     0.0474\n  Year: 2010-2014              -0.2570    5.71e-38     Make:  Plymouth          0.0452     0.0259\n  Make: Acura                  -0.3662    5.09e-78     Make:  Pontiac           0.1203  2.66e-09\n  Make: AM General              0.1799    3.94e-19     Make:  Porsche          -0.3172  6.45e-58\n  Make: Aston Martin           -0.1173      6.6e-09    Make:  Ram               0.1174  6.41e-09\n  Make: Audi                   -0.4422   6.62e-117     Make:  Rolls-Royce      -0.2056  1.33e-24\n  Make: Bentley                -0.0002        0.992    Make:  Saab             -0.3364    2.3e-65\n  Make: BMW                    -0.4488   9.54e-121     Make:  Saturn            0.1309  9.42e-11\n  Make: Buick                   0.1495    1.28e-13     Make:  Scion            -0.0187      0.358\n  Make: Cadillac                0.1507    8.26e-14     Make:  Smart            -0.2235  6.88e-29\n  Make: Chevrolet               0.3650    1.81e-77     Make:  Subaru           -0.1721  1.32e-17\n  Make: Chrysler                0.0675    0.000878     Make:  Suzuki           -0.0502     0.0133\n  Make: Daewoo                 -0.0027        0.893    Make:  Tesla            -0.0891    1.1e-05\n  Make: Dodge                   0.4096    5.97e-99     Make:  Toyota           -0.2077  4.24e-25\n  Make: Eagle                   0.0409       0.0438    Make:  Volkswagen       -0.3294  1.31e-62\n  Make: Ferrari                -0.1439    1.02e-12     Make:  Volvo            -0.2526  1.08e-36\nTable S7. Pearson r correlation coefficients and their associated p-\nvalues for each car attribute between the percentage of residents\nwith a some amount of college-level education and each car attribute,\nat the zip code level.\n50 of 58                  Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n  Variable                  Pearson’s r       p-value    Variable             Pearson’s r    p-value\n  Price                         0.4762    9.32e-138      Make:  Fiat              0.1001   7.63e-07\n  Cars/Image                    0.1769      1.53e-18     Make:  Fisker            0.1594   2.75e-15\n  MPG Highway                   0.4188    8.61e-104      Make:  Ford             -0.4310  1.68e-110\n  MPG City                      0.4928    6.96e-149      Make:  Geo              -0.1954   2.48e-22\n  Hybrid                        0.1709      2.16e-17     Make:  GMC              -0.3045   2.61e-53\n  Electric                      0.2644       3.8e-40     Make:  Honda             0.5044  5.55e-157\n  Foreign                       0.5761    7.19e-215      Make:  Hummer            0.0296       0.145\n  Country: England              0.4858    3.69e-144      Make:  Hyundai           0.2421   9.39e-34\n  Country: Germany              0.5990    1.41e-236      Make:  Infiniti          0.3671     2.2e-78\n  Country: Italy                0.2677      3.81e-41     Make:  Isuzu            -0.1013   5.61e-07\n  Country: Japan                0.5101    4.13e-161      Make:  Jaguar            0.0730   0.000314\n  Country: South Korea          0.2224      1.28e-28     Make:  Jeep              0.1450   6.77e-13\n  Country: Sweden               0.4482    2.03e-120      Make:  Kia               0.0877   1.51e-05\n  Country: USA                 -0.5761    7.19e-215      Make:  Lamborghini       0.2240   5.22e-29\n  Body Type: Convertible        0.2604       5.8e-39     Make:  Land Rover        0.3980   4.81e-93\n  Body Type: Coupe             -0.1435      1.17e-12     Make:  Lexus             0.4678  2.14e-132\n  Body Type: Crew Cab          -0.1952      2.66e-22     Make:  Lincoln          -0.1939   5.15e-22\n  Body Type: Extended Cab      -0.3285         3e-62     Make:  Lotus             0.1168   7.61e-09\n  Body Type: Hatchback          0.5563    1.63e-197      Make:  Maserati          0.1667   1.33e-16\n  Body Type: Minivan           -0.0054          0.789    Make:  Maybach           0.0604    0.00292\n  Body Type: Regular Cab       -0.4164    1.62e-102      Make:  Mazda             0.3960   4.65e-92\n  Body Type: Sedan              0.0435        0.0322     Make:  Mclaren           0.1201   2.86e-09\n  Body Type: SUV                0.3200      5.67e-59     Make:  Mercedes-Benz     0.4196  2.85e-104\n  Body Type: Van               -0.1251      6.18e-10     Make:  Mercury          -0.2760   9.64e-44\n  Body Type: Wagon              0.4356    4.55e-113      Make:  Mini              0.4333  8.48e-112\n  Year: 1990-1994              -0.4318    5.94e-111      Make:  Mitsubishi       -0.0962   2.04e-06\n  Year: 1995-1999              -0.4575    5.71e-126      Make:  Nissan            0.0588    0.00375\n  Year: 2000-2004               0.2168      3.01e-27     Make:  Oldsmobile       -0.3223   7.67e-60\n  Year: 2005-2009               0.4910    1.18e-147      Make:  Panoz             0.1190   4.04e-09\n  Year: 2010-2014               0.4297    8.57e-110      Make:  Plymouth         -0.2432   4.71e-34\n  Make: Acura                   0.3970      1.45e-92     Make:  Pontiac          -0.2527   1.01e-36\n  Make: AM General             -0.1202      2.76e-09     Make:  Porsche           0.3995   8.61e-94\n  Make: Aston Martin            0.1597      2.35e-15     Make:  Ram              -0.0559    0.00582\n  Make: Audi                    0.5247    4.77e-172      Make:  Rolls-Royce       0.1492   1.43e-13\n  Make: Bentley                -0.0555       0.00625     Make:  Saab              0.3839   3.61e-86\n  Make: BMW                     0.4894    1.41e-146      Make:  Saturn            0.0338      0.0959\n  Make: Buick                  -0.3060      7.86e-54     Make:  Scion             0.2404   2.71e-33\n  Make: Cadillac               -0.3432      3.96e-68     Make:  Smart             0.2721   1.68e-42\n  Make: Chevrolet              -0.5134    1.48e-163      Make:  Subaru            0.3973   1.04e-92\n  Make: Chrysler               -0.2066       7.9e-25     Make:  Suzuki            0.0255       0.209\n  Make: Daewoo                 -0.0122          0.548    Make:  Tesla             0.1318   7.03e-11\n  Make: Dodge                  -0.5082    8.88e-160      Make:  Toyota            0.4168  9.85e-103\n  Make: Eagle                  -0.1532      3.08e-14     Make:  Volkswagen        0.5222  4.34e-170\n  Make: Ferrari                 0.1161       9.6e-09     Make:  Volvo             0.4221  1.32e-105\nTable S8. Pearson r correlation coefficients and their associated p-\nvalues for each car attribute between the percentage of residents\nwith a bachelor’s degree and each car attribute, at the zip code level.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei    51 of 58\n\n  Variable                  Pearson’s r     p-value    Variable             Pearson’s r    p-value\n  Price                          0.4799  3.15e-140     Make:  Fiat              0.1333   4.16e-11\n  Cars/Image                     0.2212   2.65e-28     Make:  Fisker            0.2006   1.81e-23\n  MPG Highway                    0.4867  9.11e-145     Make:  Ford             -0.4457  6.03e-119\n  MPG City                       0.5451  3.44e-188     Make:  Geo              -0.2069   6.79e-25\n  Hybrid                         0.2256   2.05e-29     Make:  GMC              -0.3500   6.06e-71\n  Electric                       0.3236   2.45e-60     Make:  Honda             0.5075  3.17e-159\n  Foreign                        0.5775  3.65e-216     Make:  Hummer            0.0311       0.125\n  Country: England               0.5434  8.86e-187     Make:  Hyundai           0.2812   2.12e-45\n  Country: Germany               0.6397  8.72e-280     Make:  Infiniti          0.3845   1.91e-86\n  Country: Italy                 0.3448     8.8e-69    Make:  Isuzu            -0.1266   3.78e-10\n  Country: Japan                 0.4903  3.53e-147     Make:  Jaguar            0.1406   3.33e-12\n  Country: South Korea           0.2560   1.16e-37     Make:  Jeep              0.1283   2.21e-10\n  Country: Sweden                0.5266  1.77e-173     Make:  Kia               0.0941   3.34e-06\n  Country: USA                  -0.5775  3.65e-216     Make:  Lamborghini       0.2799   5.56e-45\n  Body Type: Convertible         0.2634   7.39e-40     Make:  Land Rover        0.4297  8.43e-110\n  Body Type: Coupe              -0.1454   6.02e-13     Make:  Lexus             0.4415  1.86e-116\n  Body Type: Crew Cab           -0.2882   1.05e-47     Make:  Lincoln          -0.1016   5.15e-07\n  Body Type: Extended Cab       -0.4198  2.35e-104     Make:  Lotus             0.1358    1.8e-11\n  Body Type: Hatchback           0.5848  6.04e-223     Make:  Maserati          0.2035   3.99e-24\n  Body Type: Minivan            -0.0024       0.905    Make:  Maybach           0.0808   6.63e-05\n  Body Type: Regular Cab        -0.4727   1.7e-135     Make:  Mazda             0.3948   1.91e-91\n  Body Type: Sedan               0.1410   2.89e-12     Make:  Mclaren           0.1495   1.27e-13\n  Body Type: SUV                 0.2720   1.79e-42     Make:  Mercedes-Benz     0.4664  1.53e-131\n  Body Type: Van                -0.0593    0.00345     Make:  Mercury          -0.1878   1.01e-20\n  Body Type: Wagon               0.4746  9.77e-137     Make:  Mini              0.4604  9.23e-128\n  Year: 1990-1994               -0.4526  4.82e-123     Make:  Mitsubishi       -0.1283   2.21e-10\n  Year: 1995-1999               -0.4365  1.25e-113     Make:  Nissan            0.0365     0.0721\n  Year: 2000-2004                0.1785   7.62e-19     Make:  Oldsmobile       -0.2907   1.59e-48\n  Year: 2005-2009                0.5065  1.59e-158     Make:  Panoz             0.1316   7.47e-11\n  Year: 2010-2014                0.4501  1.58e-121     Make:  Plymouth         -0.2022   7.88e-24\n  Make: Acura                    0.4350  9.08e-113     Make:  Pontiac          -0.2142    1.3e-26\n  Make: AM General              -0.1582   4.44e-15     Make:  Porsche           0.4353  6.14e-113\n  Make: Aston Martin             0.2038   3.39e-24     Make:  Ram              -0.0590    0.00363\n  Make: Audi                     0.5867  8.92e-225     Make:  Rolls-Royce       0.2186   1.11e-27\n  Make: Bentley                 -0.0199       0.326    Make:  Saab              0.4599  1.93e-127\n  Make: BMW                      0.5301   3.5e-176     Make:  Saturn            0.0157       0.441\n  Make: Buick                   -0.2406   2.42e-33     Make:  Scion             0.1821   1.49e-19\n  Make: Cadillac                -0.3113   9.66e-56     Make:  Smart             0.3045   2.64e-53\n  Make: Chevrolet               -0.5376  4.31e-182     Make:  Subaru            0.4357  4.02e-113\n  Make: Chrysler                -0.1708   2.34e-17     Make:  Suzuki            0.0211       0.298\n  Make: Daewoo                   0.0039       0.849    Make:  Tesla             0.1434   1.22e-12\n  Make: Dodge                   -0.5311  5.83e-177     Make:  Toyota            0.3773   4.59e-83\n  Make: Eagle                   -0.1099   5.66e-08     Make:  Volkswagen        0.5278  2.34e-174\n  Make: Ferrari                  0.1898   3.87e-21     Make:  Volvo             0.4940  1.02e-149\nTable S9. Pearson r correlation coefficients and their associated p-\nvalues for each car attribute between the percentage of residents\nwith a graduate or professional degree and each car attribute, at the\nzip code level.\n52 of 58                  Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n               %Less Than High School                      %High School                            %Some College\n  Rank    Variable              Pearson’s r    Variable                 Pearson’s r    Variable                  Pearson’s r\n     1    Year: 1995-1999           0.3715     Country: USA                 0.6048     Body Type: Extended Cab       0.4391\n     2    Year: 1990-1994           0.3230     Make: Dodge                  0.5527     Body Type: Regular Cab        0.4194\n     3    Make: Chevrolet           0.3183     Make: Chevrolet              0.4747     Make: Dodge                   0.4096\n     4    Make: Ford                0.2991     Make: Buick                  0.4488     Country: USA                  0.3834\n     5    Country: USA              0.2955     Year: 1995-1999              0.4220     Make: Chevrolet               0.3650\n    84    Make: Hyundai            -0.3468     Make: Honda                 -0.5371     Cars/Image                   -0.4257\n    85    Country: Sweden          -0.3541     Body Type: Hatchback        -0.5576     Make: Audi                   -0.4422\n    86    Make: Subaru             -0.3597     Country: Japan              -0.5670     Country: England             -0.4480\n    87    Year: 2005-2009          -0.3831     Foreign                     -0.6048     Make: BMW                    -0.4488\n    88    Body Type: Wagon         -0.3888     Country: Germany            -0.6142     Country: Germany             -0.4791\n                   %Bachelor’s Degree                       %Graduate Degree\n  Rank    Variable                 Pearson’s r    Variable                   Pearson’s r\n     1    Country: Germany              0.5990    Country: Germany                0.6397\n     2    Foreign                       0.5761    Make: Audi                      0.5867\n     3    Body Type: Hatchback          0.5563    Body Type: Hatchback            0.5848\n     4    Make: Audi                    0.5247    Foreign                         0.5775\n     5    Make: Volkswagen              0.5222    MPG City                        0.5451\n    84    Year: 1990-1994              -0.4318    Year: 1990-1994                -0.4526\n    85    Year: 1995-1999              -0.4575    Body Type: Regular Cab         -0.4727\n    86    Make: Dodge                  -0.5082    Make: Dodge                    -0.5311\n    87    Make: Chevrolet              -0.5134    Make: Chevrolet                -0.5376\n    88    Country: USA                 -0.5761    Country: USA                   -0.5775\nTable S10. The five car attributes that correlate most positively and\nmost negatively with the percentage of each education level in zip\ncode.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei                            53 of 58\n\n  Variable                   Pearson’s r      p-value Variable             Pearson’s r   p-value\n  Price                           0.2182    1.39e-27  Make:  Fiat              0.0370     0.0679\n  Cars/Image                     -0.1478    2.42e-13  Make:  Fisker            0.0113      0.579\n  MPG Highway                    -0.0753    0.000205  Make:  Ford              0.0559   0.00582\n  MPG City                       -0.0008        0.967 Make:  Geo              -0.0557   0.00606\n  Hybrid                         -0.0068        0.739 Make:  GMC               0.0536   0.00819\n  Electric                        0.0560     0.00575  Make:  Honda             0.0045      0.823\n  Foreign                         0.0358       0.0778 Make:  Hummer            0.0877  1.49e-05\n  Country: England                0.0422       0.0376 Make:  Hyundai           0.1538  2.46e-14\n  Country: Germany                0.0572     0.00478  Make:  Infiniti         -0.1273  3.03e-10\n  Country: Italy                  0.0290        0.153 Make:  Isuzu            -0.0218      0.283\n  Country: Japan                  0.0075        0.712 Make:  Jaguar           -0.2120  4.21e-26\n  Country: South Korea            0.1635    4.96e-16  Make:  Jeep              0.2893    4.6e-48\n  Country: Sweden                 0.0749    0.000219  Make:  Kia               0.1149  1.34e-08\n  Country: USA                   -0.0358       0.0778 Make:  Lamborghini       0.0407     0.0451\n  Body Type: Convertible          0.0727    0.000332  Make:  Land Rover        0.0686  0.000718\n  Body Type: Coupe               -0.1948    3.29e-22  Make:  Lexus            -0.0263      0.194\n  Body Type: Crew Cab             0.1986    4.98e-23  Make:  Lincoln          -0.3003    7.7e-52\n  Body Type: Extended Cab         0.2041    2.97e-24  Make:  Lotus             0.0438     0.0308\n  Body Type: Hatchback            0.1702    2.95e-17  Make:  Maserati         -0.0308      0.129\n  Body Type: Minivan             -0.0093        0.647 Make:  Maybach          -0.0355       0.08\n  Body Type: Regular Cab          0.1237    9.42e-10  Make:  Mazda             0.0734  0.000295\n  Body Type: Sedan               -0.4181   1.84e-103  Make:  Mclaren           0.0066      0.745\n  Body Type: SUV                  0.3053    1.37e-53  Make:  Mercedes-Benz    -0.1011  5.86e-07\n  Body Type: Van                 -0.1390    6.01e-12  Make:  Mercury          -0.2581  2.76e-38\n  Body Type: Wagon                0.2153    7.21e-27  Make:  Mini              0.1182  5.13e-09\n  Year: 1990-1994                -0.1668    1.28e-16  Make:  Mitsubishi       -0.0663   0.00107\n  Year: 1995-1999                -0.2599    8.08e-39  Make:  Nissan           -0.1289    1.8e-10\n  Year: 2000-2004                 0.1514    6.23e-14  Make:  Oldsmobile       -0.2065  8.01e-25\n  Year: 2005-2009                 0.2104        1e-25 Make:  Panoz             0.0313      0.123\n  Year: 2010-2014                 0.1864    1.97e-20  Make:  Plymouth         -0.1291  1.69e-10\n  Make: Acura                    -0.0497       0.0142 Make:  Pontiac          -0.1455  5.67e-13\n  Make: AM General                0.0649     0.00137  Make:  Porsche           0.0839  3.48e-05\n  Make: Aston Martin              0.0098        0.629 Make:  Ram               0.0635   0.00174\n  Make: Audi                      0.1198    3.18e-09  Make:  Rolls-Royce      -0.0234       0.25\n  Make: Bentley                  -0.1299    1.29e-10  Make:  Saab              0.1443  8.86e-13\n  Make: BMW                      -0.0357       0.0789 Make:  Saturn            0.0590   0.00359\n  Make: Buick                    -0.2529    8.89e-37  Make:  Scion             0.1415  2.42e-12\n  Make: Cadillac                 -0.3535    1.93e-72  Make:  Smart             0.1069  1.29e-07\n  Make: Chevrolet                 0.0097        0.632 Make:  Subaru            0.2397  4.34e-33\n  Make: Chrysler                 -0.2134    1.96e-26  Make:  Suzuki            0.0349     0.0855\n  Make: Daewoo                   -0.0324        0.111 Make:  Tesla             0.0339     0.0946\n  Make: Dodge                     0.0648     0.00139  Make:  Toyota            0.0034      0.867\n  Make: Eagle                    -0.1144    1.55e-08  Make:  Volkswagen        0.1827  1.09e-19\n  Make: Ferrari                  -0.0134        0.509 Make:  Volvo             0.0531   0.00887\nTable S11. Pearson r correlation coefficients and their associated\np-values for each car attribute between the percentage of white resi-\ndents and each car attribute, at the zip code level.\n54 of 58                   Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n  Variable                   Pearson’s r      p-value  Variable             Pearson’s r    p-value\n  Price                          -0.1895      4.4e-21  Make:  Fiat             -0.0250       0.217\n  Cars/Image                     -0.0710    0.000459   Make:  Fisker            0.0304       0.134\n  MPG Highway                     0.0465       0.0218  Make:  Ford              0.0432       0.033\n  MPG City                       -0.0832    4.05e-05   Make:  Geo               0.0196       0.335\n  Hybrid                          0.0466       0.0217  Make:  GMC               0.0281       0.167\n  Electric                       -0.0942    3.32e-06   Make:  Honda            -0.2304   1.24e-30\n  Foreign                        -0.2580    2.97e-38   Make:  Hummer           -0.0968   1.76e-06\n  Country: England               -0.0640     0.00161   Make:  Hyundai           0.0248       0.221\n  Country: Germany               -0.1650    2.75e-16   Make:  Infiniti          0.0334      0.0998\n  Country: Italy                 -0.0033          0.87 Make:  Isuzu            -0.0659    0.00116\n  Country: Japan                 -0.2855    8.43e-47   Make:  Jaguar            0.2494   8.76e-36\n  Country: South Korea            0.0273        0.178  Make:  Jeep             -0.1016   5.19e-07\n  Country: Sweden                -0.0272        0.181  Make:  Kia               0.0188       0.355\n  Country: USA                    0.2580    2.97e-38   Make:  Lamborghini      -0.0171       0.399\n  Body Type: Convertible         -0.0265        0.191  Make:  Land Rover       -0.0994   9.16e-07\n  Body Type: Coupe                0.2151    7.87e-27   Make:  Lexus            -0.1338   3.58e-11\n  Body Type: Crew Cab            -0.1996    2.93e-23   Make:  Lincoln           0.4088   1.57e-98\n  Body Type: Extended Cab        -0.2583    2.52e-38   Make:  Lotus            -0.0515      0.0111\n  Body Type: Hatchback           -0.2692    1.32e-41   Make:  Maserati          0.0424      0.0366\n  Body Type: Minivan             -0.0707     0.00049   Make:  Maybach           0.0712   0.000443\n  Body Type: Regular Cab         -0.1403    3.68e-12   Make:  Mazda            -0.1301   1.22e-10\n  Body Type: Sedan                0.4421   7.52e-117   Make:  Mclaren          -0.0252       0.215\n  Body Type: SUV                 -0.2267    1.05e-29   Make:  Mercedes-Benz    -0.0204       0.314\n  Body Type: Van                  0.0799    7.98e-05   Make:  Mercury           0.4479  3.12e-120\n  Body Type: Wagon               -0.1386    6.71e-12   Make:  Mini             -0.1564   9.09e-15\n  Year: 1990-1994                 0.0795    8.65e-05   Make:  Mitsubishi        0.0176       0.386\n  Year: 1995-1999                 0.2483    1.86e-35   Make:  Nissan           -0.0285        0.16\n  Year: 2000-2004                -0.1324    5.73e-11   Make:  Oldsmobile        0.3670   2.24e-78\n  Year: 2005-2009                -0.1417    2.25e-12   Make:  Panoz             0.1011   5.96e-07\n  Year: 2010-2014                -0.1408    3.11e-12   Make:  Plymouth          0.2056     1.3e-24\n  Make: Acura                    -0.0751    0.000212   Make:  Pontiac           0.3529     3.4e-72\n  Make: AM General               -0.0647     0.00142   Make:  Porsche          -0.0925     4.9e-06\n  Make: Aston Martin              0.0086        0.671  Make:  Ram               0.0667   0.000998\n  Make: Audi                     -0.1189    4.17e-09   Make:  Rolls-Royce       0.0955   2.39e-06\n  Make: Bentley                   0.1577    5.27e-15   Make:  Saab             -0.0704   0.000516\n  Make: BMW                      -0.1151    1.29e-08   Make:  Saturn            0.0510      0.0119\n  Make: Buick                     0.4922   1.73e-148   Make:  Scion            -0.2472   3.83e-35\n  Make: Cadillac                  0.5015   6.37e-155   Make:  Smart            -0.1127   2.58e-08\n  Make: Chevrolet                 0.1058    1.71e-07   Make:  Subaru           -0.1821   1.46e-19\n  Make: Chrysler                  0.4137   4.29e-101   Make:  Suzuki           -0.0578    0.00436\n  Make: Daewoo                    0.0187        0.358  Make:  Tesla            -0.0672   0.000924\n  Make: Dodge                     0.1010    6.02e-07   Make:  Toyota           -0.3335   3.34e-64\n  Make: Eagle                     0.2003    2.04e-23   Make:  Volkswagen       -0.2372   1.97e-32\n  Make: Ferrari                   0.0199        0.326  Make:  Volvo            -0.0153       0.451\nTable S12. Pearson r correlation coefficients and their associated\np-values for each car attribute between the percentage of black resi-\ndents and each car attribute, at the zip code level.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei  55 of 58\n\n  Variable                   Pearson’s r      p-value Variable             Pearson’s r    p-value\n  Price                           0.1130    2.32e-08  Make:  Fiat             -0.0092       0.651\n  Cars/Image                      0.3179    3.54e-58  Make:  Fisker           -0.0110       0.587\n  MPG Highway                     0.2580    2.93e-38  Make:  Ford             -0.3324   8.99e-64\n  MPG City                        0.3379    5.95e-66  Make:  Geo              -0.0159       0.433\n  Hybrid                          0.0140         0.49 Make:  GMC              -0.2607     4.7e-39\n  Electric                        0.1512    6.73e-14  Make:  Honda             0.5174  1.56e-166\n  Foreign                         0.5162   1.34e-165  Make:  Hummer           -0.0058       0.775\n  Country: England                0.1797    4.47e-19  Make:  Hyundai          -0.1209   2.28e-09\n  Country: Germany                0.3445    1.14e-68  Make:  Infiniti          0.2498   6.76e-36\n  Country: Italy                  0.0566     0.00525  Make:  Isuzu             0.0369      0.0687\n  Country: Japan                  0.5727   7.39e-212  Make:  Jaguar            0.0279       0.169\n  Country: South Korea           -0.1476    2.67e-13  Make:  Jeep             -0.2754   1.51e-43\n  Country: Sweden                 0.0857    2.33e-05  Make:  Kia              -0.1391   5.78e-12\n  Country: USA                   -0.5162   1.34e-165  Make:  Lamborghini       0.0505      0.0127\n  Body Type: Convertible          0.0381      0.0602  Make:  Land Rover        0.1286   1.98e-10\n  Body Type: Coupe               -0.0233        0.251 Make:  Lexus             0.4142  2.16e-101\n  Body Type: Crew Cab            -0.1247    6.92e-10  Make:  Lincoln          -0.1779       1e-18\n  Body Type: Extended Cab        -0.1183    4.99e-09  Make:  Lotus             0.0609    0.00265\n  Body Type: Hatchback            0.3293    1.54e-62  Make:  Maserati          0.0355      0.0804\n  Body Type: Minivan              0.1799    4.09e-19  Make:  Maybach          -0.0151       0.458\n  Body Type: Regular Cab         -0.1631    5.93e-16  Make:  Mazda             0.2303   1.33e-30\n  Body Type: Sedan                0.0961    2.05e-06  Make:  Mclaren           0.0742    0.00025\n  Body Type: SUV                 -0.1002    7.39e-07  Make:  Mercedes-Benz     0.3549   4.99e-73\n  Body Type: Van                  0.0378      0.0628  Make:  Mercury          -0.2835   3.67e-46\n  Body Type: Wagon                0.0187        0.356 Make:  Mini              0.1919   1.41e-21\n  Year: 1990-1994                -0.0363      0.0739  Make:  Mitsubishi        0.0137          0.5\n  Year: 1995-1999                -0.1500    1.08e-13  Make:  Nissan            0.1730   8.91e-18\n  Year: 2000-2004                 0.0434      0.0324  Make:  Oldsmobile       -0.2771   4.28e-44\n  Year: 2005-2009                 0.0888    1.17e-05  Make:  Panoz            -0.1239     8.9e-10\n  Year: 2010-2014                 0.0879    1.43e-05  Make:  Plymouth         -0.1663   1.56e-16\n  Make: Acura                     0.3251    6.24e-61  Make:  Pontiac          -0.3100   2.91e-55\n  Make: AM General               -0.0515      0.0111  Make:  Porsche           0.1269   3.48e-10\n  Make: Aston Martin              0.0266         0.19 Make:  Ram              -0.1782   8.81e-19\n  Make: Audi                      0.1654    2.31e-16  Make:  Rolls-Royce      -0.0467      0.0213\n  Make: Bentley                  -0.0346      0.0878  Make:  Saab              0.0064       0.753\n  Make: BMW                       0.3801    2.18e-84  Make:  Saturn           -0.1075   1.08e-07\n  Make: Buick                    -0.3356    4.73e-65  Make:  Scion             0.2085   2.85e-25\n  Make: Cadillac                 -0.2621    1.86e-39  Make:  Smart             0.0740   0.000261\n  Make: Chevrolet                -0.3734    2.81e-81  Make:  Subaru            0.0245       0.226\n  Make: Chrysler                 -0.2971    1.06e-50  Make:  Suzuki            0.0179       0.378\n  Make: Daewoo                    0.0249         0.22 Make:  Tesla             0.0979   1.33e-06\n  Make: Dodge                    -0.4053    9.68e-97  Make:  Toyota            0.6340  2.59e-273\n  Make: Eagle                    -0.1243    7.72e-10  Make:  Volkswagen        0.2052   1.66e-24\n  Make: Ferrari                   0.0504      0.0129  Make:  Volvo             0.0953   2.51e-06\nTable S13. Pearson r correlation coefficients and their associated\np-values for each car attribute between the percentage of Asian resi-\ndents and each car attribute, at the zip code level.\n56 of 58                   Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n\n                       %White                                 %Black                         %Asian\n  Rank    Variable             Pearson’s r    Variable                Pearson’s r Variable          Pearson’s r\n     1    Body Type: SUV            0.3053    Make: Cadillac              0.5015  Make: Toyota          0.6340\n     2    Make: Jeep                0.2893    Make: Buick                 0.4922  Country: Japan        0.5727\n     3    Make: Subaru              0.2397    Make: Mercury               0.4479  Make: Honda           0.5174\n     4    Price                     0.2182    Body Type: Sedan            0.4421  Foreign               0.5162\n     5    Body Type: Wagon          0.2153    Make: Chrysler              0.4137  Make: Lexus           0.4142\n    84    Make: Mercury            -0.2581    Foreign                    -0.2580  Make: Ford           -0.3324\n    85    Year: 1995-1999          -0.2599    Body Type: Extended Cab    -0.2583  Make: Buick          -0.3356\n    86    Make: Lincoln            -0.3003    Body Type: Hatchback       -0.2692  Make: Chevrolet      -0.3734\n    87    Make: Cadillac           -0.3535    Country: Japan             -0.2855  Make: Dodge          -0.4053\n    88    Body Type: Sedan         -0.4181    Make: Toyota               -0.3335  Country: USA         -0.5162\nTable S14. The five car attributes that correlate most positively and\nmost negatively with the percentage of each race in a zip code.\nTimnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei               57 of 58\n\n  Variable                  Pearson’s r       p-value   Variable             Pearson’s r    p-value\n                                                −300\n  Price                          -0.2768  ≤ 10          Make:  Fiat              0.0165    0.00815\n  Cars/Image                      0.3718  ≤ 10−300      Make:  Fisker            0.0173    0.00543\n  MPG Highway                     0.3307  ≤ 10−300      Make:  Ford             -0.1746  7.41e-176\n  MPG City                        0.2597  ≤ 10−300      Make:  Geo               0.0681   6.63e-28\n  Hybrid                          0.0318     3.1e-07    Make:  GMC              -0.1675  8.16e-162\n  Electric                        0.0347    2.35e-08    Make:  Honda             0.0705   8.59e-30\n  Foreign                         0.0743    5.93e-33    Make:  Hummer           -0.0587   3.72e-21\n  Country: England                0.1159    6.69e-78    Make:  Hyundai          -0.0162    0.00938\n  Country: Germany                0.1665    7.7e-160    Make:  Infiniti          0.0772   2.08e-35\n  Country: Italy                  0.0563    1.32e-19    Make:  Isuzu             0.0369   3.12e-09\n  Country: Japan                  0.0297    1.83e-06    Make:  Jaguar            0.1180   1.09e-80\n  Country: South Korea           -0.0150      0.0158    Make:  Jeep             -0.0563   1.39e-19\n  Country: Sweden                 0.1509   1.96e-131    Make:  Kia              -0.0076         0.22\n  Country: USA                   -0.0743    5.93e-33    Make:  Lamborghini       0.0366   4.03e-09\n  Body Type: Convertible          0.0144      0.0206    Make:  Land Rover        0.0438   1.87e-12\n  Body Type: Coupe                0.1426   2.52e-117    Make:  Lexus            -0.0578     1.5e-20\n  Body Type: Crew Cab            -0.4799  ≤ 10−300      Make:  Lincoln           0.1387  3.71e-111\n  Body Type: Extended Cab        -0.4266  ≤ 10−300      Make:  Lotus             0.0147      0.0178\n  Body Type: Hatchback            0.1193     1.6e-82    Make:  Maserati          0.0291   2.94e-06\n  Body Type: Minivan              0.0524    3.38e-17    Make:  Maybach           0.0160         0.01\n  Body Type: Regular Cab         -0.3047  ≤ 10−300      Make:  Mazda             0.0801   5.49e-38\n  Body Type: Sedan                0.4829  ≤ 10−300      Make:  Mclaren           0.0301     1.3e-06\n  Body Type: SUV                 -0.2246      1e-292    Make:  Mercedes-Benz     0.0830   1.08e-40\n  Body Type: Van                  0.1154    2.73e-77    Make:  Mercury           0.1830  2.67e-193\n  Body Type: Wagon                0.1463   1.97e-123    Make:  Mini              0.0700   2.19e-29\n  Year: 1990-1994                 0.1396   1.89e-112    Make:  Mitsubishi        0.0101       0.104\n  Year: 1995-1999                 0.2950  ≤ 10−300      Make:  Nissan            0.0212   0.000668\n  Year: 2000-2004                -0.1728    3.4e-172    Make:  Oldsmobile        0.2250  9.64e-294\n  Year: 2005-2009                -0.1951   7.58e-220    Make:  Panoz             0.0128      0.0392\n  Year: 2010-2014                -0.1651   2.71e-157    Make:  Plymouth          0.1696  7.47e-166\n  Make: Acura                     0.1032    3.93e-62    Make:  Pontiac           0.2191  3.61e-278\n  Make: AM General               -0.1182    6.02e-81    Make:  Porsche           0.0089       0.153\n  Make: Aston Martin              0.0005        0.931   Make:  Ram              -0.0855   4.51e-43\n  Make: Audi                      0.0951    6.14e-53    Make:  Rolls-Royce       0.0537   6.09e-18\n  Make: Bentley                   0.0481       1e-14    Make:  Saab              0.0784   1.86e-36\n  Make: BMW                       0.1203    8.72e-84    Make:  Saturn            0.0358   8.69e-09\n  Make: Buick                     0.2177   1.57e-274    Make:  Scion            -0.1093   1.66e-69\n  Make: Cadillac                  0.1144    6.54e-76    Make:  Smart             0.0141      0.0232\n  Make: Chevrolet                -0.1842   8.44e-196    Make:  Subaru            0.1414  1.72e-115\n  Make: Chrysler                  0.1708   2.51e-168    Make:  Suzuki            0.0396   1.98e-10\n  Make: Daewoo                    0.0453    3.09e-13    Make:  Tesla            -0.0059       0.347\n  Make: Dodge                    -0.1010    1.43e-59    Make:  Toyota           -0.0545   1.97e-18\n  Make: Eagle                     0.0776    9.26e-36    Make:  Volkswagen        0.1529  8.78e-135\n  Make: Ferrari                   0.0463    1.01e-13    Make:  Volvo             0.1442  5.31e-120\nTable S15. Pearson r correlation coefficients and their associated\np-values between each car attribute and %Obama. The variables\n“Price”, “MPG City”, and “MPG Highway” are calculated as expected\nvalues for each precinct, and all other variables are expressed as a\npercent of all cars observed in each precinct.\n58 of 58                  Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, Li Fei-Fei\n"
```

</div>



## Google searching

<div class="layout-chunk" data-layout="l-body">

```r
doi <- googlesearchR(as.character(r_ph[51, "DOI"], n = 1))

doi[1]
```

```
[[1]]
[1] "https://www.ncbi.nlm.nih.gov/pubmed/29183967"
```

```r
get_page_text(as.character(doi[1]))
```

```
 [1] "\n\tWarning:\n\tThe NCBI web site requires JavaScript to function. \n\tmore...\n\t"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
 [2] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
 [3] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
 [4] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
 [5] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
 [6] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
 [7] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
 [8] "Generate a file for use with external citation management software."                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
 [9] ""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
[10] "The United States spends more than $250 million each year on the American Community Survey (ACS), a labor-intensive door-to-door study that measures statistics relating to race, gender, education, occupation, unemployment, and other demographic factors. Although a comprehensive source of data, the lag between demographic changes and their appearance in the ACS can exceed several years. As digital imagery becomes ubiquitous and machine vision techniques improve, automated data analysis may become an increasingly practical supplement to the ACS. Here, we present a method that estimates socioeconomic characteristics of regions spanning 200 US cities by using 50 million images of street scenes gathered with Google Street View cars. Using deep learning-based computer vision techniques, we determined the make, model, and year of all motor vehicles encountered in particular neighborhoods. Data from this census of motor vehicles, which enumerated 22 million automobiles in total (8% of all automobiles in the United States), were used to accurately estimate income, race, education, and voting patterns at the zip code and precinct level. (The average US precinct contains ∼1,000 people.) The resulting associations are surprisingly simple and powerful. For instance, if the number of sedans encountered during a drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next presidential election (88% chance); otherwise, it is likely to vote Republican (82%). Our results suggest that automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time."
[11] "computer vision; deep learning; demography; social analysis"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
[12] "The authors declare no conflict of interest."                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
[13] "\n                \n                    National Center for\n                        Biotechnology Information,\n                 U.S. National Library of Medicine\n                \n                    8600 Rockville Pike, Bethesda\n                    MD, 20894\n                    USA\n                \n            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
```

</div>


## Searching Pubmed

<div class="layout-chunk" data-layout="l-body">


</div>




### Searching Medline




