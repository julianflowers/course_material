---
title: "ML literature in PH"
subtitle: "Exercise using unsupervised machine learning"
author: "Julian Flowers"
date: "1 February 2019"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Introduction

In this exercise we are going to look at trends in machine learning algorithms used in the public health literature. We are going to search [Pubmed](https://www.ncbi.nlm.nih.gov/pubmed/) which is a large, comprehensive, and freely availalble bibliogaphic database of references with a powerful search syntax.

## Objectives

1. Learn how to download PubMed abstracts to R using the `pubmedAbstractR` function in the `myScrapers` package.
2. Learn how to perform a basic natural processing pipeline using the `tidytext` and `quanteda` package
3. Use unsupervised machine learning to classify abstracts and try and extract themes and trends from abstract titles

This exercise is based on https://projects.datacamp.com/projects/158.

### First step

We need to install relevant packages if not already installed. `tidytext` and `quanteda` can be installed from CRAN. `myScrapers` is available from Github. We'll also use `dplyr` for data wrangling and `ggplot2` for visualisation and `stm` for topic modelling.

Topic modelling is a form of unsupervised machine learning used to classify texts. In natural language processing, you turn text into data you can analyse in the form out a table of word counts per document, and the clustering of words within and between documents can be analysed to find themes (topics) within and between your texts.

For more resources see the suggested reading list at the end of the exercise.

```{r}
if(!require("myScrapers"))
devtools::install_github("julianflowers/myScrapers")
library(myScrapers)
library(dplyr)
library(tidytext)
library(ggplot2)
library(quanteda)
library(stm)


```


### Downloading abstracts

To do this we need to pass 4 arguments to the `pubmedAbstractR` function:

* a `search` term - this can take full Pubmed syntax (it is built on top of `RISmed`) (See [here](https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.PubMed_Quick_Start) for further details.
* a `start` year
* an `end` year
* `n` = number of abstracts to be downloaded. We recommend using n = 1 initially - this will return how many abstracts are available to be downloaded and you can an adjust the value of n accordingly. The default value is 1000.

(You can also extract authors by setting author = TRUE, and keywords (MeSH tags) by setting keyword = TRUE. For this exercise we suggest leaving the default  = FALSE for both as it returns a much smaller table).

### Example search 

As a taster try the following search.

```{r}
search <- "randomforest public health"
start <- 2010
end <- 2018
n <- 1

result <- pubmedAbstractR(search = search, start = start, end = end, n = n)

```

We can see this search retrieves 20 titles - we can download all for inspection

```{r}
n <- 20

rf <- pubmedAbstractR(search = search, start = start, end = end, n = n)

head(rf, 10)



```

## Your turn

Try searching for and downloading machine learning articles in public health over the last 10 years. (Hint: using public health[mh] in your search string, restricts the search to MeSH headings ). Note: it takes a few minutes to download the abstracts. You can build up very complex searches. For more information see [here](https://www.ncbi.nlm.nih.gov/pubmed/advanced)

HINT: https://gist.github.com/julianflowers/2ca8a062e86b7752bacbc66609314d95

```{r, cache=TRUE, echo=FALSE, results="hide"}

start <- 2008
end <- 2019
n <- 5547
search <- "machine learning[mh] public health[mh]"
check <- pubmedAbstractR(search = search, n = n, start = start, end = end)

```

### Simple NLP pipeline

There is a fairly standard approach:

* Ingest your text
* Clean texts - remove unwanted text e.g. stopwords (common English words like *the, a, at, in* etc.), remove punctuation, remove html tags and urls
* Create a *corpus* - that is a body of texts - this is a speciliased  data format which holds the raw text along with metadata - this is necessary for some package like `quanteda`.
* Tokenise - that is split texts into basic units - this can be anything from single characters to paragraphs. The usual approach is to split texts into words - this is known as a *bag of words*. A single word is known as a 1-gram; if we use two word combinations, this is *bigram*, 3 word is a 3-gram and so on. Combinations of non consecutive words are known as *skip grams*.
* Create a **document term matrix** or **document feature matrix**. This is the basis of further analysis, and is a table where the rows are your documents, the columns are your ngrams (terms) and the cells are counts of each term in each document. This matrix will be *sparse*  - that is full of 0s, and can have large numbers of columns (sometimes billions).

We are going tos.

1. *Tokenise* the title of each article into 2 word combinations (bigrams) (because it'll make it easier to extract algorithm names). You can achieve this with the `create_bigrams` function in `myScrapers` or you can build it from scratch (https://www.tidytextmining.com/ngrams.html), and count token frequency.

**Tokenise the titles of your downloaded abstracts and visualise your result showing the most frequent bigrams over time**

```{r fig.height=4, fig.width=12, results="hide"}

bigrams <- check %>%
  create_bigrams(title)
  
bigrams %>%
  count(bigram, year) %>%
  dplyr::top_n(100) %>%
  ggplot(aes(bigram, fct_rev(year), fill = n)) +
  geom_tile() +
  theme(axis.text.x= element_text(size = 7, angle = 90, hjust = 0)) +
  scale_x_discrete(position = "top") +
  scale_fill_distiller(palette = "Spectral")

```

Visualise your result.

HINT: https://gist.github.com/julianflowers/19ee08b2873c2142a26f2e85b6d0b4a1

What conclusions do you draw?



## Topic modelling

Topic modelling is a form of unsupervised machine learning intended to automatically find themes in bodies of texts. 


To perform topic modelling we need to:

* Convert our data to a matrix format known as a document-term-matrix (DTM) or a sparese matrix.
* For this exercise we will use the `stm` (Structural topic modelling) package and apply 15 topics (note this is soemwhat arbitrary - see further reading for techniques to determine statistical optima for numbers of topic models).
* We'll also use single word models rather than bigrams.

Follow the code below:

This does the following

1. Creates a document-feature matrix using `tidytext` - we have split our abstracts into words, removed words we don't want and then counted the frequency of words per abstract and 'cast' the result into the correct data format for further analysis.
2. Set a random seed - this makes the analysis more reprodicible because the algorithm starts at the same point every time. If you change the seed number you might get a different result.
3. Used the `stm` package to apply its algorithm to detect 15 topics. You can change this and see how the output varies. If you want to see what the model is doing set `verbose` = TRUE.

This may take a few minutes to run.

```{r}
library(stm); library(stmCorrViz)

big_dfm <- check %>%
  unnest_tokens(word, title) %>%
  anti_join(get_stopwords()) %>%
  count(DOI, word, sort = TRUE) %>%
  cast_dfm(DOI, word, n )
  
seed <- set.seed(42)  

topic_model <- stm(big_dfm, K = 15, seed = seed,
                   verbose = FALSE, init.type = "Spectral")

```



the `stm` has a number of useful plotting options to visualise your results. **Try plotting your results and creating a word cloud**. Type ??stm if you need help

```{r, results="hide" }

plot(topic_model, n = 5, text.cex = 0.8)

```


```{r, results = "hide"}
topic_model %>% cloud()
```



Now we can "tidy" the model outputs usng the `tidy` function to extract some of the results. This extracts the `r expression(beta)` coefficients which represents the probability of a given work being in a given topic. By extracting words (terms) with the highest `r expression(beta)` values, we can create a set of keyowrds for that topic and infer themes.

```{r}

tidy_tm <- tidy(topic_model)


tidy_tm
  
```
To clarify we can visualise the data to show terms with the highest probablity for each topic.

**Create a visualisation to show the top 10 terms in each of the 15 topics. Label the topics to reflect the terms**

HINT: https://gist.github.com/julianflowers/f9200cf0da8dbbad20a2285f6d8f170a

```{r fig.height=10, results = "hide" }
#install_github("dgrtwo/drlib")

library(drlib)

tidy_tm %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic), 
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales ="free") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with different topics") +
  phecharts::theme_phe() +
  ggsave("ml_ph_themes.png")
```

We can look at how these topics vary over time. To this we need to assign a topic number to each abstract. Abstracts may have more than one topic but we will assign the one with highest probability for that abstract. This value is known as the `r expression(gamma)` coefficient and can be extracted as below.

```{r}

top_terms <- tidy_tm %>%
  group_by(topic) %>%
  arrange(-beta)%>%
  slice(1:10) %>%
  mutate(kw = paste(term, collapse = ", ")) %>%
  select(topic, kw) %>%
  distinct()

td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(big_dfm)) %>%
  group_by(document) %>%
  dplyr::filter(gamma == max(gamma)) %>%
  left_join(check, by = c("document" = "DOI"))
```

**Take the output of `td_gamma` and plot topics over time**

```{r, results = "hide"}

td_gamma %>%
  select(-document) %>%
  dplyr::filter(year != 2019) %>%
  left_join(top_terms) %>%
  group_by(year) %>%
  count(kw) %>%
  ggplot(aes(year, n, fill = factor(kw))) +
  geom_col(show.legend = FALSE) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  facet_wrap(~ kw, ncol = 3, labeller = label_wrap_gen(40)) +
  labs(title = "Trend in machine learning topics",
       y = "Number of articles", x = "Year") +
  phecharts::theme_phe() +
  theme(strip.text.x = element_text(size = rel(.6)))



```

## Questions

* What are the downsides to topic modelling?
* How might you improve your results?

## Further reading

https://en.wikipedia.org/wiki/Topic_model
https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/
https://www.theoj.org/joss-papers/joss.00774/10.21105.joss.00774.pdf (Quanteda)
https://kenbenoit.net/pdfs/text_analysis_in_R.pdf ** STRONGLY RECOMMENDED
https://www.tidytextmining.com/ ** STRONGLY RECOMMENDED


### Advanced

Try running some of the code below for more options

#### Relationship between topics

```{r}
topic_corr <- topicCorr(topic_model)
plot(topic_corr)

  
```


#### Online visualisations

```{r}
remotes::install_github("mroberts/stmBrowser",dependencies=TRUE)
install.packages("stminsights")
library(stmBrowser); library(stminsights)

run_stminsights()
```


```{r, eval=FALSE}

stmCorrViz::stmCorrViz(prevfit, file = "test_1.html", documents_raw = check$abstract,  documents_matrix = data = meta, text = "abstract")

```



```{r}
stmCorrViz::stmCorrViz(mod = topic_model, file_out = "example.html", documents_raw = check$title, documents_matrix = big_dfm)


summary(topic_model)
```
